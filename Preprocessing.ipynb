{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“Œ <span style=\"font-size:18px; color:#007acc;\"><b>Introduction</b></span>\n",
        "\n",
        "This notebook demonstrates a modular pipeline for loan delinquency prediction using data from Freddie Mac. It includes well-structured functions for data loading, merging, handling missing values through mask generation, applying scalable unsupervised outlier detection techniques and EDA. All processing functions are defined before execution, with external variables managed via a YAML configuration file to ensure flexibility and reproducibility. As the workflow was developed on Microsoft Azure, certain settings may need to be adjusted when running the notebook in a different environment. The dataset was obtained from **[Freddie Mac](https://www.freddiemac.com/)**. \n",
        "\n",
        "<span style=\"font-size:18px; color:#e63946;\"><b>\n"
      ],
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "id": "d90b70b9-5d9c-45e0-afbf-566331ea453e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span style=\"font-size:18px; color:#007acc;\"><b> Table of Contents</b></span>\n",
        "1. [Install libraries](#Install-Libraries)\n",
        "2. [Import modules](#Import-Modules) \n",
        "3. [Functions for data pre-processing](#Functions-for-data-pre-processing)  \n",
        "4. [Main execution code for pre-processing](#Main-execution-code-for-pre-processing)  \n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "72ecf068-8921-4007-8be5-4125d5e13e19"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span style=\"font-size:18px; color:#007acc;\"><b> 1. Install libraries <a id=\"Install-Libraries\"></a> ##"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "0a08a020-408e-4583-939b-2b6b657581cd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Required libraries to be installed\n",
        "!pip install numpy \n",
        "!pip install pandas\n",
        "!pip install pyod  # PyOD for anomaly detection (outlier detection algorithms)\n",
        "!pip install seaborn \n",
        "!pip install category_encoders  # Category Encoders for transforming categorical data into numeric\n",
        "!pip install suod  # SUOD for scalable unsupervised outlier detection (for large datasets)\n",
        "!pip install openpyxl\n",
        "!pip install matplotlib\n",
        "!pip install azureml-core  # AzureML Core for interacting with Azure Machine Learning services\n",
        "!pip install azure-storage-blob  # Azure Storage Blob for managing files in Azure Blob Storage\n",
        "!pip install azureml-dataset-runtime  # AzureML Dataset Runtime for working with datasets in Azure ML"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744463828200
        }
      },
      "id": "c410abb8-5774-42ef-bef9-436e60ea7a63"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span style=\"font-size:18px; color:#007acc;\"><b> 2. Import modules <a id=\"Import-modules\"></a> ##"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "79480d14-4d6a-4798-87cd-4884e9695ef9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "from io import BytesIO\n",
        "\n",
        "# Third-party imports\n",
        "import category_encoders as ce\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyod\n",
        "import scipy\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import yaml\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "from pyod.models.hbos import HBOS\n",
        "from pyod.models.knn import KNN\n",
        "from pyod.models.lof import LOF\n",
        "from pyod.models.ocsvm import OCSVM\n",
        "from pyod.models.iforest import IForest\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.preprocessing import (\n",
        "    LabelEncoder,\n",
        "    MinMaxScaler,\n",
        "    Normalizer,\n",
        "    OneHotEncoder,\n",
        "    RobustScaler,\n",
        "    StandardScaler,\n",
        ")\n",
        "from suod.models.base import SUOD"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744463842849
        }
      },
      "id": "b5c06d29-df5e-4144-b822-8903f35e8319"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span style=\"font-size:18px; color:#007acc;\"><b> 3. Functions for data pre-processing <a id=\"Functions for data-preprocessing\"></a> ##"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "d0d86384-6981-4808-8171-5f0a2f319171"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The following code consists of the following functions for data pre-processing:\n",
        "\n",
        "- `load_config()`: Loads the configuration from a YAML file.\n",
        "- `connect_to_azure_datastore()`: Establishes a connection to Azure Blob Storage.\n",
        "- `read_data_from_blob()`: Reads loan origination and performance data.\n",
        "- `merge_data()`: Merge loan origination and performance data.\n",
        "- `detect_missing_values()`: Detects missing values based on predefined placeholders and extra conditions.\n",
        "- `load_extra_conditions()`: Loads custom conditions for missing value handling from the config file.\n",
        "- `clean_column_using_mask()`: Cleans columns based on a mask identifying missing values.\n",
        "- `apply_one_hot_encoding()`: Applies one-hot encoding to categorical columns.\n",
        "- `detect_and_plot_outliers()`: Detects and plots outliers in the dataset.\n",
        "- `visualize_and_check_data()`: Visualizes and checks data quality before training.\n",
        "\n",
        "Parameters:\n",
        "- `config (dict)`: Configuration dictionary containing paths, columns, thresholds, and other parameters.\n",
        "- `container_client (ContainerClient)`: Azure Blob Storage container client for accessing the data.\n",
        "\n",
        "Returns:\n",
        "- The processed dataset with separated features and labels, ready for machine learning or further analysis.\n",
        "\"\"\"\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "\n",
        "def load_config(config_path=\"config.yaml\"):\n",
        "    \"\"\"\n",
        "    Load the YAML configuration file.\n",
        "\n",
        "    Args:\n",
        "        config_path (str): Path to the YAML configuration file. Defaults to \"config.yaml\".\n",
        "\n",
        "    Returns:\n",
        "        dict: Parsed configuration dictionary.\n",
        "    \"\"\"\n",
        "    logging.info(\"Loading configuration file...\")\n",
        "    with open(config_path, \"r\") as file:\n",
        "        return yaml.safe_load(file)\n",
        "\n",
        "\n",
        "def connect_to_azure_datastore(config):\n",
        "    \"\"\"\n",
        "    Connect to Azure Blob Storage using the provided configuration.\n",
        "\n",
        "    Args:\n",
        "        config (dict): Configuration dictionary containing 'connection_str' and 'container_name'.\n",
        "\n",
        "    Returns:\n",
        "        azure.storage.blob.ContainerClient: Azure container client object.\n",
        "    \"\"\"\n",
        "    logging.info(\"Connecting to Azure datastore...\")\n",
        "    connection_str = config[\"datastore\"][\"connection_str\"]\n",
        "    container_name = config[\"datastore\"][\"container_name\"]\n",
        "    blob_service_client = BlobServiceClient.from_connection_string(connection_str)\n",
        "    return blob_service_client.get_container_client(container_name)\n",
        "\n",
        "\n",
        "def read_data_from_blob(container_client, file_path, columns, relevant_columns, identifier):\n",
        "    \"\"\"\n",
        "    Read data from a blob and return it as a cleaned and filtered DataFrame.\n",
        "\n",
        "    Args:\n",
        "        container_client (ContainerClient): Azure Blob container client.\n",
        "        file_path (str): Path to the blob file.\n",
        "        columns (list): List of column names to assign to the DataFrame.\n",
        "        relevant_columns (list): Subset of columns to retain.\n",
        "        identifier (str): Column name to be used as the primary identifier for sorting and ordering.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Processed DataFrame with relevant columns and duplicates removed.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Fetching data from {file_path}...\")\n",
        "    try:\n",
        "        blob_client = container_client.get_blob_client(file_path)\n",
        "        blob_data = blob_client.download_blob().readall()\n",
        "        df = pd.read_csv(BytesIO(blob_data), sep=\"|\")\n",
        "        df.columns = columns  # Rename columns\n",
        "        df = df[[identifier] + [col for col in df.columns if col != identifier]]  # Reorder columns\n",
        "        df = df[relevant_columns]\n",
        "        df.drop_duplicates(subset=relevant_columns[0], keep=\"last\", inplace=True)  # Remove duplicates\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to process {file_path}: {e}\")\n",
        "        return pd.DataFrame()  # Return an empty DataFrame in case of error\n",
        "\n",
        "\n",
        "def merge_data(container_client, config):\n",
        "    \"\"\"\n",
        "    Read origination and performance data from Azure Blob Storage, process, and merge them.\n",
        "\n",
        "    Args:\n",
        "        container_client (ContainerClient): Azure container client to access blobs.\n",
        "        config (dict): Configuration dictionary with file paths, columns, and identifiers.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Merged and processed DataFrame combining origination and performance data.\n",
        "    \"\"\"\n",
        "    identifier = config[\"loan_identifier\"]\n",
        "    origination_file_path = config[\"parameters\"][\"origination_file_path\"]\n",
        "    performance_file_path = config[\"parameters\"][\"performance_file_path\"]\n",
        "\n",
        "    # Read origination data\n",
        "    origination_data = read_data_from_blob(\n",
        "        container_client,\n",
        "        origination_file_path,\n",
        "        config[\"columns\"][\"origination\"],\n",
        "        config[\"columns\"][\"relevant_origination_data\"],\n",
        "        identifier\n",
        "    )\n",
        "\n",
        "    # Read performance data\n",
        "    performance_data = read_data_from_blob(\n",
        "        container_client,\n",
        "        performance_file_path,\n",
        "        config[\"columns\"][\"performance\"],\n",
        "        config[\"columns\"][\"relevant_performance_data\"],\n",
        "        identifier\n",
        "    )\n",
        "\n",
        "    # Merge data\n",
        "    logging.info(\"Merging origination and performance data...\")\n",
        "    if not origination_data.empty and not performance_data.empty:\n",
        "        df = pd.merge(origination_data, performance_data, on=identifier, how=\"inner\")\n",
        "        logging.info(\"Data processing completed successfully.\")\n",
        "        print(df.head())\n",
        "        return df\n",
        "    else:\n",
        "        logging.error(\"One or both datasets are empty. Merging skipped.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "def detect_missing_values(df, extra_conditions=None):\n",
        "    \"\"\"\n",
        "    Detect missing values in a DataFrame using null checks, placeholder strings, and optional custom conditions.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame to check for missing values.\n",
        "        extra_conditions (dict, optional): Dictionary mapping column names to functions that define custom missing value logic.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame of the same shape with 1s indicating missing values and 0s otherwise.\n",
        "    \"\"\"\n",
        "    missing_mask = df.isnull()\n",
        "\n",
        "    common_placeholders = {'', ' ', 'NA', 'N/A', 'null', 'None', 'none', 'nan', 'NaN', 'UNKNOWN', 'unk', 'UNK'}\n",
        "    \n",
        "    for column in df.select_dtypes(include=['object']).columns:\n",
        "        missing_mask[column] |= df[column].astype(str).str.strip().isin(common_placeholders)\n",
        "\n",
        "    if extra_conditions:\n",
        "        for column, condition_fn in extra_conditions.items():\n",
        "            if column in df.columns:\n",
        "                try:\n",
        "                    missing_mask[column] |= condition_fn(df[column])\n",
        "                except Exception as e:\n",
        "                    logging.warning(f\"Error applying condition on {column}: {e}\")\n",
        "\n",
        "    return missing_mask.astype(int)\n",
        "\n",
        "\n",
        "def plot_missing_data(mask):\n",
        "    \"\"\"\n",
        "    Plot a horizontal bar chart showing the count of missing values per column.\n",
        "\n",
        "    Args:\n",
        "        mask (pd.DataFrame): A DataFrame of 0s and 1s indicating non-missing and missing values respectively.\n",
        "\n",
        "    Returns\n",
        "        None: Displays a plot summarizing missing data distribution across columns.\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure(figsize=(8, 6)) \n",
        "\n",
        "    # Bar plot of missing values per column\n",
        "    missing_counts = mask.sum()\n",
        "    missing_counts = missing_counts[missing_counts > 0]  # Only keep columns with missing values\n",
        "    \n",
        "    if not missing_counts.empty:\n",
        "        plt.barh(missing_counts.index, missing_counts.values, color=\"darkred\")  # Horizontal bar plot\n",
        "        plt.title(\"Missing Values Per Column\")\n",
        "        plt.xlabel(\"Count of Missing Values\")  # Change x-label to match horizontal plot\n",
        "        plt.ylabel(\"Columns\")  # Add y-label for the column names\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, \"No Missing Values\", fontsize=12, ha=\"center\", va=\"center\")\n",
        "        plt.yticks([])  # Remove y-ticks if no data is plotted\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def load_extra_conditions(config):\n",
        "    \"\"\"\n",
        "    Load extra missing value conditions from a YAML configuration file.\n",
        "\n",
        "    The conditions are defined based on FreddieMac documentation and map column names to custom logic.\n",
        "\n",
        "    Args:\n",
        "        config_path (str): Path to the YAML configuration file containing extra conditions.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary mapping column names to condition functions.\n",
        "    \"\"\"\n",
        "\n",
        "    extra_conditions = {}\n",
        "    for column, condition in config.get(\"missing_value_conditions\", {}).items():\n",
        "        try:\n",
        "            extra_conditions[column] = eval(f\"lambda x: {condition}\", {\"np\": np, \"pd\": pd})\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Error parsing condition for {column}: {e}\")\n",
        "    return extra_conditions    \n",
        "\n",
        "\n",
        "def clean_column_using_mask(df, column, updated_mask, config=None):\n",
        "    \"\"\"\n",
        "    Clean missing values in a specified column using a mask and predefined imputation strategies.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame to clean.\n",
        "        column (str): The name of the column to clean.\n",
        "        updated_mask (pd.DataFrame): A DataFrame mask indicating missing values (1 for missing, 0 otherwise).\n",
        "        config (dict, optional): Configuration specifying imputation strategy (e.g., 'mean' or 'mode') for the column.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with the specified column cleaned and mask updated.\n",
        "    \"\"\"\n",
        "    # Identify the indices where the updated mask indicates missing values for the column\n",
        "    indices = updated_mask.index[updated_mask[column] == 1]\n",
        "    \n",
        "    if config and 'missing_value_replacement' in config:\n",
        "        # Mode replacement\n",
        "        if column in config['missing_value_replacement'].get('mode', []):\n",
        "            mode_value = df[column].mode()\n",
        "            if not mode_value.empty:\n",
        "                # Fill only at the indices indicated by the mask\n",
        "                df.loc[indices, column] = df.loc[indices, column].fillna(mode_value[0])\n",
        "                # Update the updated mask: mark these as no longer missing\n",
        "                updated_mask.loc[indices, column] = 0\n",
        "\n",
        "        # Mean replacement (for numeric columns only)\n",
        "        elif column in config['missing_value_replacement'].get('mean', []):\n",
        "            if pd.api.types.is_numeric_dtype(df[column]):\n",
        "                df.loc[indices, column] = df.loc[indices, column].fillna(df[column].mean())\n",
        "                updated_mask.loc[indices, column] = 0\n",
        "\n",
        "    # Return the updated DataFrame\n",
        "    return df\n",
        "\n",
        "\n",
        "def apply_one_hot_encoding(df):\n",
        "    \"\"\"\n",
        "    Identify categorical columns (excluding the first) and apply One-Hot Encoding (OHE).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame containing categorical and numerical columns.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A new DataFrame with OHE applied to eligible categorical columns, excluding those with mixed types.\n",
        "    \"\"\"\n",
        "    # Identify categorical columns, excluding the first column\n",
        "    categorical_columns = df.iloc[:, 1:].select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    \n",
        "    one_hot_encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
        "    \n",
        "    for col in categorical_columns:\n",
        "        \n",
        "        # Check if the column has mixed data types (int and str)\n",
        "        if df[col].apply(lambda x: isinstance(x, (str, int))).all():\n",
        "            if df[col].apply(type).nunique() > 1:  # Check if types are mixed\n",
        "                continue  # Skip the column\n",
        "        \n",
        "        # Apply one-hot encoding to the valid column\n",
        "        encoded_data = one_hot_encoder.fit_transform(df[[col]])\n",
        "        encoded_df = pd.DataFrame(encoded_data, \n",
        "                                  columns=[f\"{col}_{i}\" for i in range(encoded_data.shape[1])])\n",
        "        \n",
        "        # Drop the original column\n",
        "        df.drop(columns=[col], inplace=True)\n",
        "        \n",
        "        # Concatenate the encoded columns with the original DataFrame\n",
        "        df = pd.concat([df, encoded_df], axis=1)\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def detect_and_plot_outliers(df, config_path=\"config.yaml\", contamination=None):\n",
        "    \"\"\"\n",
        "    Detect outliers in a DataFrame using ensemble anomaly detection models defined in a YAML config.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame containing features for anomaly detection.\n",
        "        config_path (str): Path to the YAML configuration file that includes anomaly detection settings.\n",
        "        contamination (float, optional): Expected proportion of outliers in the data. Defaults to 0.001.\n",
        "\n",
        "    Returns:\n",
        "        tuple:\n",
        "            - pd.DataFrame: Original DataFrame with average outlier scores appended.\n",
        "            - pd.DataFrame: Filtered DataFrame excluding detected outliers based on average scores.\n",
        "    \"\"\"        \n",
        "    # --- Load configuration data from YAML ---\n",
        "    with open(config_path, \"r\") as file:\n",
        "        config = yaml.safe_load(file)\n",
        "\n",
        "    # Use contamination from config if not explicitly passed\n",
        "    if contamination is None:\n",
        "        contamination = config.get(\"contamination\", 0.001)    \n",
        "\n",
        "    # Get anomaly detection columns from the config\n",
        "    anomaly_inputs = config.get(\"columns\", {}).get(\"anomaly_detection\", [])\n",
        "\n",
        "    if not anomaly_inputs:\n",
        "        raise ValueError(\"No anomaly detection columns found in config.\")\n",
        "\n",
        "    X_train = merged_df[anomaly_inputs]\n",
        "    \n",
        "    print(\"step1\")\n",
        "    # Set up your base estimators\n",
        "    base_estimators = [\n",
        "        #IForest(n_estimators=200),\n",
        "        IForest(n_estimators=100),\n",
        "        #LOF(n_neighbors=5, contamination=contamination),\n",
        "        #LOF(n_neighbors=15, contamination=contamination),\n",
        "        #LOF(n_neighbors=25, contamination=contamination),\n",
        "        HBOS(contamination=contamination),\n",
        "        #OCSVM(contamination=contamination),\n",
        "        #KNN(n_neighbors=5, contamination=contamination),\n",
        "        #KNN(n_neighbors=15, contamination=contamination),\n",
        "        #KNN(n_neighbors=25, contamination=contamination)\n",
        "    ]  \n",
        "\n",
        "    # Initialize SUOD model\n",
        "    model = SUOD(base_estimators=base_estimators, n_jobs=1,\n",
        "             rp_flag_global=True, bps_flag=True,\n",
        "             approx_flag_global=True, contamination=contamination)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(X_train)\n",
        "\n",
        "    # conduct model approximation if it is enabled\n",
        "    model.approximate(X_train)  \n",
        "    \n",
        "    # Predict labels for each base estimator\n",
        "    predicted_labels = model.predict(X_train)\n",
        "\n",
        "    # Calculate the average of predicted labels across all estimators\n",
        "    average_labels = np.mean(predicted_labels, axis=1)\n",
        "\n",
        "    # Add the average labels to X_train\n",
        "    X_train_with_avg_labels = np.hstack((X_train, average_labels.reshape(-1, 1)))\n",
        "\n",
        "    # Separate outliers and inliers based on the average labels\n",
        "    outliers_avg = X_train_with_avg_labels[average_labels >= 0.5]\n",
        "    inliers_avg = X_train_with_avg_labels[average_labels < 0.5]\n",
        "\n",
        "    def convert_to_float(value):\n",
        "        try:\n",
        "            return float(value)\n",
        "        except ValueError:\n",
        "            try:\n",
        "                return float(value.replace(\"'\", \"\"))  # Handle string representations of floats\n",
        "            except ValueError:\n",
        "                return np.nan  # Handle non-convertible values\n",
        "\n",
        "    # Convert values to float\n",
        "    inliers_avg = np.vectorize(convert_to_float)(inliers_avg)\n",
        "    outliers_avg = np.vectorize(convert_to_float)(outliers_avg)\n",
        "\n",
        "    # Unique values in last column\n",
        "    unique_inliers_last_col = np.unique(inliers_avg[:, -1])\n",
        "    unique_outliers_last_col = np.unique(outliers_avg[:, -1])\n",
        "\n",
        "    # Construct DataFrame with average labels\n",
        "    df_OD = pd.DataFrame(average_labels)\n",
        "    merged_df_OD = pd.concat([merged_df, df_OD], axis=1)\n",
        "    df_merged_OD = pd.DataFrame(merged_df_OD)\n",
        "    df_merged_OD_WO_Outliers = df_merged_OD[df_merged_OD.iloc[:, -1] < 0.5]\n",
        "    last_column = df_merged_OD_WO_Outliers.columns[-1]\n",
        "    return merged_df_OD, df_merged_OD_WO_Outliers\n",
        "\n",
        "\n",
        "def visualize_and_check_data(df, columns=None, config=None):\n",
        "    \"\"\"\n",
        "    Visualize and perform basic checks on a DataFrame, including distribution plots for numerical columns \n",
        "    and anomaly detection statistics if a configuration is provided.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to be visualized and checked.\n",
        "        columns (list, optional): A list of columns to consider for analysis. If None, all numerical columns are used.\n",
        "        config (dict, optional): A configuration dictionary that can include columns for anomaly detection.\n",
        "            If provided, anomaly detection statistics and histograms will be displayed for the specified columns.\n",
        "\n",
        "    Returns:\n",
        "        None: This function prints output directly and displays plots, but does not return any values.\n",
        "    \"\"\"\n",
        "    # Check if the dataframe is empty\n",
        "    if df.empty:\n",
        "        print(\"The dataframe is empty!\")\n",
        "        return\n",
        "    \n",
        "    # 1. Check basic information about the dataset (optional)\n",
        "    # print(\"Dataset Info:\")\n",
        "    # print(df.info())\n",
        "    \n",
        "    # 2. Summary statistics of numerical columns (optional)\n",
        "    # print(\"\\nSummary Statistics:\")\n",
        "    # print(df.describe())\n",
        "    \n",
        "    # Determine which columns to include (either from 'columns' parameter or all numerical columns)\n",
        "    if columns is None:\n",
        "        numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "    else:\n",
        "        # Ensure specified columns exist in the dataframe\n",
        "        numerical_cols = [col for col in columns if col in df.columns]\n",
        "    \n",
        "    # Test plot to verify if the issue is with plotting\n",
        "    if not numerical_cols:\n",
        "        print(\"No numerical columns found for plotting.\")\n",
        "        return\n",
        "    \n",
        "    # 3. Distribution of specified columns using matplotlib\n",
        "    for col in numerical_cols:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        df[col].hist(bins=50, color='skyblue', edgecolor='black')\n",
        "        plt.title(f'Distribution of {col}')\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.show()\n",
        "\n",
        "    # If config is provided, perform anomaly detection checks\n",
        "    if config:\n",
        "        # Get anomaly detection columns from the config\n",
        "        anomaly_inputs = config.get(\"columns\", {}).get(\"anomaly_detection\", [])\n",
        "\n",
        "        if not anomaly_inputs:\n",
        "            print(\"No anomaly detection columns found in config.\")\n",
        "            return\n",
        "        \n",
        "        # Check if all columns in anomaly_inputs are present in the dataframe\n",
        "        missing_columns = [col for col in anomaly_inputs if col not in df.columns]\n",
        "        if missing_columns:\n",
        "            print(f\"Missing columns in dataframe: {', '.join(missing_columns)}\")\n",
        "            return\n",
        "        \n",
        "        # Display summary statistics for the selected anomaly columns\n",
        "        print(\"\\nSummary Statistics for Anomaly Detection Columns:\")\n",
        "        print(df[anomaly_inputs].describe())\n",
        "\n",
        "        # Plot histograms for all the numeric columns related to anomaly detection\n",
        "        plt.figure(figsize=(20, 15))\n",
        "        df[anomaly_inputs].hist(bins=50, figsize=(20, 15))\n",
        "        plt.suptitle(\"Histograms for Anomaly Detection Columns\", fontsize=20)\n",
        "        plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744463843223
        }
      },
      "id": "dc655182-b9ad-44b8-bb53-1496818a3ce4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span style=\"font-size:18px; color:#007acc;\"><b> 4. Main execution code for pre-processing <a id=\"Main Execution Code for Pre-processing\"></a> ##"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "b6d66c91-379f-4556-b836-874b22a7a175"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Executes the end-to-end data processing flow, including configuration loading, data cleaning, feature engineering, \n",
        "anomaly detection, and data normalization, followed by saving the processed data to Excel files.\n",
        "\n",
        "Steps:\n",
        "    1. Load configuration from a YAML file.\n",
        "    2. Connect to Azure Blob Storage to access data.\n",
        "    3. Merge data from multiple sources.\n",
        "    4. Load additional conditions from the config for missing value detection.\n",
        "    5. Detect missing values and visualize missing data.\n",
        "    6. Clean columns using a missing data mask, updating the mask and the DataFrame.\n",
        "    7. Append the missing data mask to the DataFrame.\n",
        "    8. Apply One-Hot Encoding to categorical columns.\n",
        "    9. Apply binarization to the 'Current loan delinquency status' column based on predefined criteria.\n",
        "    10. Detect and plot outliers in the data.\n",
        "    11. Normalize anomaly detection columns using MinMaxScaler.\n",
        "    12. Drop the identifier column.\n",
        "    13. Separate features and labels into distinct DataFrames.\n",
        "    14. Save the processed features and labels to Excel files in the current working directory.\n",
        "\n",
        "Args:\n",
        "    All necessary configurations are loaded from the YAML file.\n",
        "\n",
        "Returns:\n",
        "    The processed DataFrames are saved to Excel files in the current working directory.\n",
        "\"\"\"\n",
        "\n",
        "# Start of the execution flow\n",
        "\n",
        "# Load the configuration\n",
        "config = load_config(\"config.yaml\")\n",
        "\n",
        "# Connect to Azure Blob Storage\n",
        "container_client = connect_to_azure_datastore(config)\n",
        "\n",
        "# Process data from blob storage and merge datasets\n",
        "merged_df = merge_data(container_client, config)\n",
        "\n",
        "# Load extra conditions\n",
        "extra_conditions = load_extra_conditions(config)\n",
        "\n",
        "# Create missing data mask\n",
        "missing_data_mask = detect_missing_values(merged_df, extra_conditions=extra_conditions)\n",
        "plot_missing_data(missing_data_mask)\n",
        "print(\"Number of missing values (zeros) before cleaning:\\n\", missing_data_mask.sum().sum())\n",
        "\n",
        "# Keep the original mask for training; create a copy for updating\n",
        "original_missing_data_mask = missing_data_mask.copy()\n",
        "updated_missing_data_mask = missing_data_mask.copy()\n",
        "\n",
        "# Apply cleaning on columns having missing values\n",
        "for column in merged_df.columns:\n",
        "    merged_df = clean_column_using_mask(merged_df, column, updated_missing_data_mask, config=config)\n",
        "print(\"Missing values per column after cleaning:\")\n",
        "print(merged_df.isnull().sum())\n",
        "\n",
        "# Append missing value mask to the dataframe\n",
        "# Add suffix to avoid column name conflicts\n",
        "missing_data_mask_renamed = missing_data_mask.add_suffix(\"_missing\")\n",
        "# Ensure index alignment and append\n",
        "merged_df = merged_df.join(missing_data_mask_renamed)\n",
        "\n",
        "# Apply one hot encoding\n",
        "merged_df = apply_one_hot_encoding(merged_df)\n",
        "\n",
        "# Apply binarization\n",
        "merged_df['Current loan delinquency status'] = merged_df['Current loan delinquency status'].apply(\n",
        "    lambda x: 1 if (\n",
        "        str(x).strip().lower() == config['loan_delinquency_indicator'].lower() or\n",
        "        (str(x).replace('.', '', 1).isdigit() and float(x) > config['loan_delinquency_threshold'])\n",
        "    ) else 0\n",
        ")\n",
        "\n",
        "# Detect and plot outliers\n",
        "merged_df_OD, df_wo_outliers = detect_and_plot_outliers(merged_df, config_path=\"config.yaml\")\n",
        "\n",
        "# Data quality check\n",
        "visualize_and_check_data(df_wo_outliers, columns=config[\"columns\"][\"anomaly_detection\"])\n",
        "\n",
        "# Normalize anomaly detection columns\n",
        "normalizer = MinMaxScaler()\n",
        "anomaly_cols = config[\"columns\"][\"anomaly_detection\"]\n",
        "df_wo_outliers[anomaly_cols] = normalizer.fit_transform(df_wo_outliers[anomaly_cols])\n",
        "\n",
        "# Drop identifier column\n",
        "df_wo_outliers = df_wo_outliers.drop(columns='Loan Seq Number')\n",
        "\n",
        "# Create label dataframe\n",
        "label_df = df_wo_outliers[['Current loan delinquency status']]\n",
        "\n",
        "# Drop label column from the feature dataframe\n",
        "feature_df = df_wo_outliers.drop(columns=['Current loan delinquency status'])\n",
        "\n",
        "# Print current working directory for confirmation\n",
        "print(f\" Current working directory: {os.getcwd()}\")\n",
        "\n",
        "# Write features and labels to Excel in the current directory (same as notebook)\n",
        "input_output_files = [\n",
        "    ('feature_file.xlsx', feature_df),\n",
        "    ('label_file.xlsx', label_df)\n",
        "]\n",
        "\n",
        "for file_name, data_frame in input_output_files:\n",
        "    data_frame.to_excel(file_name, index=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744465711678
        }
      },
      "id": "4bac61f8-6d8d-41e5-8c85-11fe6edcac2e"
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}