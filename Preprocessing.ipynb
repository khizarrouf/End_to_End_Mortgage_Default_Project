{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“Œ <span style=\"font-size:18px; color:#007acc;\"><b>Introduction</b></span>\n",
        "\n",
        "This notebook demonstrates a modular pipeline for loan delinquency prediction using data from Freddie Mac. It includes well-structured functions for data loading, merging, handling missing values through mask generation, applying scalable unsupervised outlier detection techniques and EDA. All processing functions are defined before execution, with external variables managed via a YAML configuration file to ensure flexibility and reproducibility. As the workflow was developed on Microsoft Azure, certain settings may need to be adjusted when running the notebook in a different environment. The dataset was obtained from **[Freddie Mac](https://www.freddiemac.com/)**. \n",
        "\n",
        "<span style=\"font-size:18px; color:#e63946;\"><b>\n"
      ],
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "id": "d90b70b9-5d9c-45e0-afbf-566331ea453e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span style=\"font-size:18px; color:#007acc;\"><b> Table of Contents</b></span>\n",
        "1. [Install libraries](#Install-Libraries)\n",
        "2. [Import modules](#Import-Modules) \n",
        "3. [Functions for data pre-processing](#Functions-for-data-pre-processing)  \n",
        "4. [Main execution code for pre-processing](#Main-execution-code-for-pre-processing)  \n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "72ecf068-8921-4007-8be5-4125d5e13e19"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span style=\"font-size:18px; color:#007acc;\"><b> 1. Install libraries <a id=\"Install-Libraries\"></a> ##"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "0a08a020-408e-4583-939b-2b6b657581cd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Required libraries to be installed\n",
        "!pip install numpy \n",
        "!pip install pandas\n",
        "!pip install pyod  # PyOD for anomaly detection (outlier detection algorithms)\n",
        "!pip install seaborn \n",
        "!pip install category_encoders  # Category Encoders for transforming categorical data into numeric\n",
        "!pip install suod  # SUOD for scalable unsupervised outlier detection (for large datasets)\n",
        "!pip install openpyxl\n",
        "!pip install matplotlib\n",
        "!pip install azureml-core  # AzureML Core for interacting with Azure Machine Learning services\n",
        "!pip install azure-storage-blob  # Azure Storage Blob for managing files in Azure Blob Storage\n",
        "!pip install azureml-dataset-runtime  # AzureML Dataset Runtime for working with datasets in Azure ML"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744395703606
        }
      },
      "id": "c410abb8-5774-42ef-bef9-436e60ea7a63"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span style=\"font-size:18px; color:#007acc;\"><b> 2. Import modules <a id=\"Import-modules\"></a> ##"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "79480d14-4d6a-4798-87cd-4884e9695ef9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "from io import BytesIO\n",
        "\n",
        "# Third-party imports\n",
        "import category_encoders as ce\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyod\n",
        "import scipy\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import yaml\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "from pyod.models.hbos import HBOS\n",
        "from pyod.models.knn import KNN\n",
        "from pyod.models.lof import LOF\n",
        "from pyod.models.ocsvm import OCSVM\n",
        "from pyod.models.iforest import IForest\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.preprocessing import (\n",
        "    LabelEncoder,\n",
        "    MinMaxScaler,\n",
        "    Normalizer,\n",
        "    OneHotEncoder,\n",
        "    RobustScaler,\n",
        "    StandardScaler,\n",
        ")\n",
        "from suod.models.base import SUOD"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744399762209
        }
      },
      "id": "b5c06d29-df5e-4144-b822-8903f35e8319"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span style=\"font-size:18px; color:#007acc;\"><b> 3. Functions for data pre-processing <a id=\"Functions for data-preprocessing\"></a> ##"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "d0d86384-6981-4808-8171-5f0a2f319171"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The workflow includes the following key steps:\n",
        "\n",
        "1. **Load Configuration**: The configuration settings (file paths, column names, etc.) are loaded from a YAML configuration file (`config.yaml`).\n",
        "2. **Connect to Azure Blob Storage**: A connection to Azure Blob Storage is established using the provided configuration.\n",
        "3. **Merge Data**: Loan origination and performance data are read from Azure Blob Storage and merged into a single dataset.\n",
        "4. **Data Processing**:\n",
        "   - **Missing Values Detection**: Identify missing values using a custom mask and visualize missing data before and after cleaning.\n",
        "   - **Missing Values Handling**: Columns with missing data are cleaned based on a mask, and the cleaning process is applied.\n",
        "   - **One-Hot Encoding**: Categorical variables are one-hot encoded for model readiness.\n",
        "   - **Outlier Detection and Removal**: Outliers in the dataset are detected, followed by outlier removal.\n",
        "5. **Anomaly Detection**: Anomaly detection columns are normalized using MinMax scaling.\n",
        "6. **Feature and Label Separation**: Features and labels are separated for model training. The label column (`Current loan delinquency status`) is flipped (0 â†’ 1, 1 â†’ 0).\n",
        "7. **Data Export**: Features and labels are exported to Excel files for further use or modeling.\n",
        "\n",
        "Functions included:\n",
        "- `load_config()`: Loads the configuration from a YAML file.\n",
        "- `connect_to_azure_datastore()`: Establishes a connection to Azure Blob Storage.\n",
        "- `read_data_from_blob()`: Reads loan origination and performance data.\n",
        "- `merge_data()`: Merge loan origination and performance data.\n",
        "- `detect_missing_values()`: Detects missing values based on predefined placeholders and extra conditions.\n",
        "- `load_extra_conditions()`: Loads custom conditions for missing value handling from the config file.\n",
        "- `clean_column_using_mask()`: Cleans columns based on a mask identifying missing values.\n",
        "- `apply_one_hot_encoding()`: Applies one-hot encoding to categorical columns.\n",
        "- `detect_and_plot_outliers()`: Detects and plots outliers in the dataset.\n",
        "- `visualize_and_check_data()`: Visualizes and checks data quality before training.\n",
        "- `MinMaxScaler`: Scales anomaly detection columns using MinMax scaling.\n",
        "\n",
        "Parameters:\n",
        "- `config (dict)`: Configuration dictionary containing paths, columns, thresholds, and other parameters.\n",
        "- `container_client (ContainerClient)`: Azure Blob Storage container client for accessing the data.\n",
        "\n",
        "Returns:\n",
        "- The processed dataset with separated features and labels, ready for machine learning or further analysis.\n",
        "\"\"\"\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "\n",
        "def load_config(config_path=\"config.yaml\"):\n",
        "    \"\"\"\n",
        "    Load the YAML configuration file.\n",
        "\n",
        "    Args:\n",
        "        config_path (str): Path to the YAML configuration file. Defaults to \"config.yaml\".\n",
        "\n",
        "    Returns:\n",
        "        dict: Parsed configuration dictionary.\n",
        "    \"\"\"\n",
        "    logging.info(\"Loading configuration file...\")\n",
        "    with open(config_path, \"r\") as file:\n",
        "        return yaml.safe_load(file)\n",
        "\n",
        "\n",
        "def connect_to_azure_datastore(config):\n",
        "    \"\"\"\n",
        "    Connect to Azure Blob Storage using the provided configuration.\n",
        "\n",
        "    Args:\n",
        "        config (dict): Configuration dictionary containing 'connection_str' and 'container_name'.\n",
        "\n",
        "    Returns:\n",
        "        azure.storage.blob.ContainerClient: Azure container client object.\n",
        "    \"\"\"\n",
        "    logging.info(\"Connecting to Azure datastore...\")\n",
        "    connection_str = config[\"datastore\"][\"connection_str\"]\n",
        "    container_name = config[\"datastore\"][\"container_name\"]\n",
        "    blob_service_client = BlobServiceClient.from_connection_string(connection_str)\n",
        "    return blob_service_client.get_container_client(container_name)\n",
        "\n",
        "\n",
        "def read_data_from_blob(container_client, file_path, columns, relevant_columns, identifier):\n",
        "    \"\"\"\n",
        "    Read data from a blob and return it as a cleaned and filtered DataFrame.\n",
        "\n",
        "    Args:\n",
        "        container_client (ContainerClient): Azure Blob container client.\n",
        "        file_path (str): Path to the blob file.\n",
        "        columns (list): List of column names to assign to the DataFrame.\n",
        "        relevant_columns (list): Subset of columns to retain.\n",
        "        identifier (str): Column name to be used as the primary identifier for sorting and ordering.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Processed DataFrame with relevant columns and duplicates removed.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Fetching data from {file_path}...\")\n",
        "    try:\n",
        "        blob_client = container_client.get_blob_client(file_path)\n",
        "        blob_data = blob_client.download_blob().readall()\n",
        "        df = pd.read_csv(BytesIO(blob_data), sep=\"|\")\n",
        "        df.columns = columns  # Rename columns\n",
        "        df = df[[identifier] + [col for col in df.columns if col != identifier]]  # Reorder columns\n",
        "        df = df[relevant_columns]\n",
        "        df.drop_duplicates(subset=relevant_columns[0], keep=\"last\", inplace=True)  # Remove duplicates\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to process {file_path}: {e}\")\n",
        "        return pd.DataFrame()  # Return an empty DataFrame in case of error\n",
        "\n",
        "\n",
        "def merge_data(container_client, config):\n",
        "    \"\"\"\n",
        "    Read origination and performance data from Azure Blob Storage, process, and merge them.\n",
        "\n",
        "    Args:\n",
        "        container_client (ContainerClient): Azure container client to access blobs.\n",
        "        config (dict): Configuration dictionary with file paths, columns, and identifiers.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Merged and processed DataFrame combining origination and performance data.\n",
        "    \"\"\"\n",
        "    identifier = config[\"loan_identifier\"]\n",
        "    origination_file_path = config[\"parameters\"][\"origination_file_path\"]\n",
        "    performance_file_path = config[\"parameters\"][\"performance_file_path\"]\n",
        "\n",
        "    # Read origination data\n",
        "    origination_data = read_data_from_blob(\n",
        "        container_client,\n",
        "        origination_file_path,\n",
        "        config[\"columns\"][\"origination\"],\n",
        "        config[\"columns\"][\"relevant_origination_data\"],\n",
        "        identifier\n",
        "    )\n",
        "\n",
        "    # Read performance data\n",
        "    performance_data = read_data_from_blob(\n",
        "        container_client,\n",
        "        performance_file_path,\n",
        "        config[\"columns\"][\"performance\"],\n",
        "        config[\"columns\"][\"relevant_performance_data\"],\n",
        "        identifier\n",
        "    )\n",
        "\n",
        "    # Merge data\n",
        "    logging.info(\"Merging origination and performance data...\")\n",
        "    if not origination_data.empty and not performance_data.empty:\n",
        "        merged_df = pd.merge(origination_data, performance_data, on=identifier, how=\"inner\")\n",
        "        logging.info(\"Data processing completed successfully.\")\n",
        "        print(merged_df.head())\n",
        "        return merged_df\n",
        "    else:\n",
        "        logging.error(\"One or both datasets are empty. Merging skipped.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "def detect_missing_values(df, extra_conditions=None):\n",
        "    \"\"\"\n",
        "    Identifies missing values in a DataFrame.\n",
        "    \"\"\"\n",
        "    missing_mask = df.isnull()\n",
        "\n",
        "    common_placeholders = {'', ' ', 'NA', 'N/A', 'null', 'None', 'none', 'nan', 'NaN', 'UNKNOWN', 'unk', 'UNK'}\n",
        "    \n",
        "    for column in df.select_dtypes(include=['object']).columns:\n",
        "        missing_mask[column] |= df[column].astype(str).str.strip().isin(common_placeholders)\n",
        "\n",
        "    if extra_conditions:\n",
        "        for column, condition_fn in extra_conditions.items():\n",
        "            if column in df.columns:\n",
        "                try:\n",
        "                    missing_mask[column] |= condition_fn(df[column])\n",
        "                except Exception as e:\n",
        "                    logging.warning(f\"Error applying condition on {column}: {e}\")\n",
        "\n",
        "    return missing_mask.astype(int)\n",
        "\n",
        "\n",
        "def plot_missing_data(mask):\n",
        "    \"\"\"Visualizes missing data using only a horizontal bar plot.\"\"\"\n",
        "    plt.figure(figsize=(8, 6)) \n",
        "\n",
        "    # Bar plot of missing values per column\n",
        "    missing_counts = mask.sum()\n",
        "    missing_counts = missing_counts[missing_counts > 0]  # Only keep columns with missing values\n",
        "    \n",
        "    if not missing_counts.empty:\n",
        "        plt.barh(missing_counts.index, missing_counts.values, color=\"darkred\")  # Horizontal bar plot\n",
        "        plt.title(\"Missing Values Per Column\")\n",
        "        plt.xlabel(\"Count of Missing Values\")  # Change x-label to match horizontal plot\n",
        "        plt.ylabel(\"Columns\")  # Add y-label for the column names\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, \"No Missing Values\", fontsize=12, ha=\"center\", va=\"center\")\n",
        "        plt.yticks([])  # Remove y-ticks if no data is plotted\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def load_extra_conditions(config):\n",
        "    \"\"\"\n",
        "    Loads extra conditions from a YAML configuration. The extra conditions were taken from the FreddieMac documentation.\n",
        "    \"\"\"\n",
        "    extra_conditions = {}\n",
        "    for column, condition in config.get(\"missing_value_conditions\", {}).items():\n",
        "        try:\n",
        "            extra_conditions[column] = eval(f\"lambda x: {condition}\", {\"np\": np, \"pd\": pd})\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Error parsing condition for {column}: {e}\")\n",
        "    return extra_conditions    \n",
        "\n",
        "\n",
        "def clean_column_using_mask(df, column, updated_mask, config=None):\n",
        "    \"\"\"\n",
        "    Cleans missing values in a column based on a precomputed missing_data_mask and predefined strategies (mode or mean replacement).\n",
        "    Only fills entries flagged as missing in the mask, and updates the mask accordingly.\n",
        "    Returns the updated DataFrame.\n",
        "    \"\"\"\n",
        "    # Identify the indices where the updated mask indicates missing values for the column\n",
        "    indices = updated_mask.index[updated_mask[column] == 1]\n",
        "    \n",
        "    if config and 'missing_value_replacement' in config:\n",
        "        # Mode replacement\n",
        "        if column in config['missing_value_replacement'].get('mode', []):\n",
        "            mode_value = df[column].mode()\n",
        "            if not mode_value.empty:\n",
        "                # Fill only at the indices indicated by the mask\n",
        "                df.loc[indices, column] = df.loc[indices, column].fillna(mode_value[0])\n",
        "                # Update the updated mask: mark these as no longer missing\n",
        "                updated_mask.loc[indices, column] = 0\n",
        "\n",
        "        # Mean replacement (for numeric columns only)\n",
        "        elif column in config['missing_value_replacement'].get('mean', []):\n",
        "            if pd.api.types.is_numeric_dtype(df[column]):\n",
        "                df.loc[indices, column] = df.loc[indices, column].fillna(df[column].mean())\n",
        "                updated_mask.loc[indices, column] = 0\n",
        "\n",
        "    # Return the updated DataFrame\n",
        "    return df\n",
        "\n",
        "\n",
        "def apply_one_hot_encoding(merged_df):\n",
        "    \"\"\"\n",
        "    Identifies categorical columns (excluding the first column) and applies One-Hot Encoding (OHE).\n",
        "    New columns are named based on the existing column names with an index.\n",
        "    Skips columns with mixed data types and prints their names and start/end indices.\n",
        "    \"\"\"\n",
        "    # Identify categorical columns, excluding the first column\n",
        "    categorical_columns = merged_df.iloc[:, 1:].select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    \n",
        "    one_hot_encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
        "    \n",
        "    for col in categorical_columns:\n",
        "        \n",
        "        # Check if the column has mixed data types (int and str)\n",
        "        if merged_df[col].apply(lambda x: isinstance(x, (str, int))).all():\n",
        "            if merged_df[col].apply(type).nunique() > 1:  # Check if types are mixed\n",
        "                continue  # Skip the column\n",
        "        \n",
        "        # Apply one-hot encoding to the valid column\n",
        "        encoded_data = one_hot_encoder.fit_transform(merged_df[[col]])\n",
        "        encoded_df = pd.DataFrame(encoded_data, \n",
        "                                  columns=[f\"{col}_{i}\" for i in range(encoded_data.shape[1])])\n",
        "        \n",
        "        # Drop the original column\n",
        "        merged_df.drop(columns=[col], inplace=True)\n",
        "        \n",
        "        # Concatenate the encoded columns with the original DataFrame\n",
        "        merged_df = pd.concat([merged_df, encoded_df], axis=1)\n",
        "    \n",
        "    return merged_df\n",
        "\n",
        "\n",
        "def detect_and_plot_outliers(df, config_path=\"config.yaml\", contamination=0.001):\n",
        "    # --- Load configuration data from YAML ---\n",
        "    with open(config_path, \"r\") as file:\n",
        "        config = yaml.safe_load(file)\n",
        "\n",
        "    # Get anomaly detection columns from the config\n",
        "    anomaly_inputs = config.get(\"columns\", {}).get(\"anomaly_detection\", [])\n",
        "\n",
        "    if not anomaly_inputs:\n",
        "        raise ValueError(\"No anomaly detection columns found in config.\")\n",
        "\n",
        "    X_train = merged_df[anomaly_inputs]\n",
        "    \n",
        "    print(\"step1\")\n",
        "    # Set up your base estimators\n",
        "    base_estimators = [\n",
        "        #IForest(n_estimators=200),\n",
        "        IForest(n_estimators=100),\n",
        "        #LOF(n_neighbors=5, contamination=contamination),\n",
        "        #LOF(n_neighbors=15, contamination=contamination),\n",
        "        #LOF(n_neighbors=25, contamination=contamination),\n",
        "        HBOS(contamination=contamination),\n",
        "        #OCSVM(contamination=contamination),\n",
        "        #KNN(n_neighbors=5, contamination=contamination),\n",
        "        #KNN(n_neighbors=15, contamination=contamination),\n",
        "        #KNN(n_neighbors=25, contamination=contamination)\n",
        "    ]  \n",
        "\n",
        "    # Initialize SUOD model\n",
        "    model = SUOD(base_estimators=base_estimators, n_jobs=1,\n",
        "             rp_flag_global=True, bps_flag=True,\n",
        "             approx_flag_global=True, contamination=contamination)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(X_train)\n",
        "\n",
        "    # conduct model approximation if it is enabled\n",
        "    model.approximate(X_train)  \n",
        "    \n",
        "    # Predict labels for each base estimator\n",
        "    predicted_labels = model.predict(X_train)\n",
        "\n",
        "    # Calculate the average of predicted labels across all estimators\n",
        "    average_labels = np.mean(predicted_labels, axis=1)\n",
        "\n",
        "    # Add the average labels to X_train\n",
        "    X_train_with_avg_labels = np.hstack((X_train, average_labels.reshape(-1, 1)))\n",
        "\n",
        "    # Separate outliers and inliers based on the average labels\n",
        "    outliers_avg = X_train_with_avg_labels[average_labels >= 0.5]\n",
        "    inliers_avg = X_train_with_avg_labels[average_labels < 0.5]\n",
        "\n",
        "    def convert_to_float(value):\n",
        "        try:\n",
        "            return float(value)\n",
        "        except ValueError:\n",
        "            try:\n",
        "                return float(value.replace(\"'\", \"\"))  # Handle string representations of floats\n",
        "            except ValueError:\n",
        "                return np.nan  # Handle non-convertible values\n",
        "\n",
        "    # Convert values to float\n",
        "    inliers_avg = np.vectorize(convert_to_float)(inliers_avg)\n",
        "    outliers_avg = np.vectorize(convert_to_float)(outliers_avg)\n",
        "\n",
        "    # Unique values in last column\n",
        "    unique_inliers_last_col = np.unique(inliers_avg[:, -1])\n",
        "    unique_outliers_last_col = np.unique(outliers_avg[:, -1])\n",
        "\n",
        "    # Construct DataFrame with average labels\n",
        "    df_OD = pd.DataFrame(average_labels)\n",
        "    merged_df_OD = pd.concat([merged_df, df_OD], axis=1)\n",
        "    df_merged_OD = pd.DataFrame(merged_df_OD)\n",
        "    df_merged_OD_WO_Outliers = df_merged_OD[df_merged_OD.iloc[:, -1] < 0.5]\n",
        "    last_column = df_merged_OD_WO_Outliers.columns[-1]\n",
        "    return merged_df_OD, df_merged_OD_WO_Outliers\n",
        "\n",
        "\n",
        "def visualize_and_check_data(df, columns=None, config=None):\n",
        "    # Check if the dataframe is empty\n",
        "    if df.empty:\n",
        "        print(\"The dataframe is empty!\")\n",
        "        return\n",
        "    \n",
        "    # 1. Check basic information about the dataset (optional)\n",
        "    # print(\"Dataset Info:\")\n",
        "    # print(df.info())\n",
        "    \n",
        "    # 2. Summary statistics of numerical columns (optional)\n",
        "    # print(\"\\nSummary Statistics:\")\n",
        "    # print(df.describe())\n",
        "    \n",
        "    # Determine which columns to include (either from 'columns' parameter or all numerical columns)\n",
        "    if columns is None:\n",
        "        numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "    else:\n",
        "        # Ensure specified columns exist in the dataframe\n",
        "        numerical_cols = [col for col in columns if col in df.columns]\n",
        "    \n",
        "    # Test plot to verify if the issue is with plotting\n",
        "    if not numerical_cols:\n",
        "        print(\"No numerical columns found for plotting.\")\n",
        "        return\n",
        "    \n",
        "    # 3. Distribution of specified columns using matplotlib\n",
        "    for col in numerical_cols:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        df[col].hist(bins=50, color='skyblue', edgecolor='black')\n",
        "        plt.title(f'Distribution of {col}')\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.show()\n",
        "\n",
        "    # If config is provided, perform anomaly detection checks\n",
        "    if config:\n",
        "        # Get anomaly detection columns from the config\n",
        "        anomaly_inputs = config.get(\"columns\", {}).get(\"anomaly_detection\", [])\n",
        "\n",
        "        if not anomaly_inputs:\n",
        "            print(\"No anomaly detection columns found in config.\")\n",
        "            return\n",
        "        \n",
        "        # Check if all columns in anomaly_inputs are present in the dataframe\n",
        "        missing_columns = [col for col in anomaly_inputs if col not in df.columns]\n",
        "        if missing_columns:\n",
        "            print(f\"Missing columns in dataframe: {', '.join(missing_columns)}\")\n",
        "            return\n",
        "        \n",
        "        # Display summary statistics for the selected anomaly columns\n",
        "        print(\"\\nSummary Statistics for Anomaly Detection Columns:\")\n",
        "        print(df[anomaly_inputs].describe())\n",
        "\n",
        "        # Plot histograms for all the numeric columns related to anomaly detection\n",
        "        plt.figure(figsize=(20, 15))\n",
        "        df[anomaly_inputs].hist(bins=50, figsize=(20, 15))\n",
        "        plt.suptitle(\"Histograms for Anomaly Detection Columns\", fontsize=20)\n",
        "        plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744399762384
        }
      },
      "id": "dc655182-b9ad-44b8-bb53-1496818a3ce4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span style=\"font-size:18px; color:#007acc;\"><b> 4. Main execution code for pre-processing <a id=\"Main Execution Code for Pre-processing\"></a> ##"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "b6d66c91-379f-4556-b836-874b22a7a175"
    },
    {
      "cell_type": "code",
      "source": [
        "# Start of the execution flow\n",
        "\n",
        "# Load the configuration\n",
        "config = load_config(\"config.yaml\")\n",
        "\n",
        "# Connect to Azure Blob Storage\n",
        "container_client = connect_to_azure_datastore(config)\n",
        "\n",
        "# Process data from blob storage and merge datasets\n",
        "merged_df = merge_data(container_client, config)\n",
        "\n",
        "# Load extra conditions\n",
        "extra_conditions = load_extra_conditions(config)\n",
        "\n",
        "# Create missing data mask\n",
        "missing_data_mask = detect_missing_values(merged_df, extra_conditions=extra_conditions)\n",
        "plot_missing_data(missing_data_mask)\n",
        "print(\"Number of missing values (zeros) before cleaning:\\n\", missing_data_mask.sum().sum())\n",
        "\n",
        "# Keep the original mask for training; create a copy for updating\n",
        "original_missing_data_mask = missing_data_mask.copy()\n",
        "updated_missing_data_mask = missing_data_mask.copy()\n",
        "\n",
        "# Apply cleaning on a specific column or loop through columns\n",
        "for column in merged_df.columns:\n",
        "    merged_df = clean_column_using_mask(merged_df, column, updated_missing_data_mask, config=config)\n",
        "print(\"Missing values per column after cleaning:\")\n",
        "print(merged_df.isnull().sum())\n",
        "\n",
        "# Append missing value mask to the dataframe\n",
        "# Add suffix to avoid column name conflicts\n",
        "missing_data_mask_renamed = missing_data_mask.add_suffix(\"_missing\")\n",
        "# Ensure index alignment and append\n",
        "merged_df = merged_df.join(missing_data_mask_renamed)\n",
        "\n",
        "# Apply one hot encoding\n",
        "merged_df = apply_one_hot_encoding(merged_df)\n",
        "\n",
        "# Apply binarization\n",
        "merged_df['Current loan delinquency status'] = merged_df['Current loan delinquency status'].apply(\n",
        "    lambda x: 1 if (\n",
        "        str(x).strip().lower() == config['loan_delinquency_indicator'].lower() or\n",
        "        (str(x).replace('.', '', 1).isdigit() and float(x) > config['loan_delinquency_threshold'])\n",
        "    ) else 0\n",
        ")\n",
        "\n",
        "# Detect and plot outliers\n",
        "merged_df_OD, df_wo_outliers = detect_and_plot_outliers(merged_df, config_path=\"config.yaml\", contamination=0.001)\n",
        "\n",
        "# Data quality check\n",
        "visualize_and_check_data(df_wo_outliers, columns=config[\"columns\"][\"anomaly_detection\"])\n",
        "\n",
        "# Normalize anomaly detection columns\n",
        "normalizer = MinMaxScaler()\n",
        "anomaly_cols = config[\"columns\"][\"anomaly_detection\"]\n",
        "df_wo_outliers[anomaly_cols] = normalizer.fit_transform(df_wo_outliers[anomaly_cols])\n",
        "\n",
        "# Drop identifier column\n",
        "df_wo_outliers = df_wo_outliers.drop(columns='Loan Seq Number')\n",
        "\n",
        "# Create label dataframe with flipped values\n",
        "label_df = df_wo_outliers[['Current loan delinquency status']]\n",
        "\n",
        "# Drop label column from the feature dataframe\n",
        "feature_df = df_wo_outliers.drop(columns=['Current loan delinquency status'])\n",
        "\n",
        "# Print current working directory for confirmation\n",
        "print(f\" Current working directory: {os.getcwd()}\")\n",
        "\n",
        "# Write features and labels to Excel in the current directory (same as notebook)\n",
        "input_output_files = [\n",
        "    ('feature_file.xlsx', feature_df),\n",
        "    ('label_file.xlsx', label_df)\n",
        "]\n",
        "\n",
        "for file_name, data_frame in input_output_files:\n",
        "    data_frame.to_excel(file_name, index=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744400906264
        }
      },
      "id": "4bac61f8-6d8d-41e5-8c85-11fe6edcac2e"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get unique values for each column in merged_df\n",
        "unique_values = merged_df.apply(pd.Series.unique)\n",
        "\n",
        "# Print the unique values for each column along with the column position\n",
        "for idx, (column, values) in enumerate(unique_values.items()):\n",
        "    print(f\"Column position {idx} - Unique values in column '{column}': {values}\")\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744400908038
        }
      },
      "id": "252cd2ae-992a-40d2-bb17-1f9aa0564185"
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}