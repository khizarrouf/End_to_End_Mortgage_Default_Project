{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“Œ <span style=\"font-size:18px; color:#007acc;\"><b>Introduction</b></span>\n",
        "\n",
        "This is the second notebook in the series. The first notebook focused on data preprocessing and cleaning, including handling missing values, detecting and removing outliers, and performing exploratory data analysis (EDA). In this notebook, we continue from that point by importing necessary modules and defining custom functions and classes for model training and testing. A custom batch sampler is implemented for efficient data loading. For interpretability, SHAP analysis is performed to explain feature importance. The trained model is then saved for future use, and a FastAPI server is created to serve the model for local inference. Finally, predictions can be made by sending requests to the API. Note: This work was conducted on MS Azure, so some settings may need adjustments.\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span style=\"font-size:18px; color:#007acc;\"><b> Table of Contents</b></span>\n",
        "1. [Install libraries](#Install-Libraries)\n",
        "2. [Import modules](#Import-Modules) \n",
        "3. [Custom functions and classes for model training](#Custom-Functions-and-Classes-for-Model-Training)  \n",
        "4. [Execution code for model training](#Execution-Code-for-Model-Training)  \n",
        "5. [SHAP analysis](#SHAP-Analysis)\n",
        "6. [Creating FAST API](#Create-FAST-API)\n",
        "7. [Inference from API](#Inference-from-API)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span style=\"font-size:18px; color:#007acc;\"><b> 1. Install libraries <a id=\"Install libraries\"></a> ##"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required libraries to be installed\n",
        "!pip install numpy \n",
        "!pip install pandas\n",
        "!pip install seaborn\n",
        "!pip install matplotlib\n",
        "!pip install azureml-core  # AzureML Core for interacting with Azure Machine Learning services\n",
        "!pip install azure-storage-blob  # Azure Storage Blob for managing files in Azure Blob Storage\n",
        "!pip install azureml-dataset-runtime  # AzureML Dataset Runtime for working with datasets in Azure ML\n",
        "!pip install torch torchvision torchmetrics  # PyTorch for deep learning, torchvision for computer vision, torchmetrics for metrics\n",
        "!pip install scikit-learn pandas numpy matplotlib seaborn\n",
        "!pip install pyyaml\n",
        "!pip install mlflow  # MLflow for managing machine learning workflows\n",
        "!pip install fastapi uvicorn  # FastAPI for building web APIs\n",
        "!pip install fsspec  # fsspec for working with file systems"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744403735907
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span style=\"font-size:18px; color:#007acc;\"><b> 2. Import modules <a id=\"Import modules\"></a> ##"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard Library\n",
        "import sys\n",
        "import math\n",
        "import logging\n",
        "import datetime\n",
        "import threading\n",
        "import requests\n",
        "import yaml\n",
        "\n",
        "# PyTorch and Related\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler, random_split, TensorDataset\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchmetrics import Precision, Recall, F1Score\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve\n",
        "\n",
        "# Plotting and Visualization\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Data Handling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# API\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from uvicorn import run\n",
        "\n",
        "# Remote File Access\n",
        "import fsspec\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744403736225
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span style=\"font-size:18px; color:#007acc;\"><b> 3. Custom functions and classes for model training <a id=\"Custom functions and classes for model training\"></a> ##"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "This code defines a custom PyTorch dataset, neural network model, and several utility functions for training, validation, and evaluation of a binary classification model.\n",
        "\n",
        "1. `LoadDataset`: A custom dataset class to load tabular data from feature and label files, with support for YAML-based configuration.\n",
        "2. `NNModel`: A neural network model with customizable layers, weight initialization, and dropout.\n",
        "3. `ProportionalBatchSampler`: A custom batch sampler to ensure proportional sampling based on state labels.\n",
        "4. Functions for training, validation, accuracy computation, ROC curve computation, and precision-recall curve computation are also defined.\n",
        "\n",
        "Each class and function is documented with arguments, return types, and functionality explained.\n",
        "\n",
        "PEP 8 recommendations have been followed, with the exception of line length exceeding 79 characters in some instances. \n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class LoadDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for loading tabular data from feature and label files,\n",
        "    optionally using a YAML config for cleaner initialization.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feature_file=None, label_file=None, sep='\\t', skiprows=1, config_path=None):\n",
        "        \"\"\"\n",
        "        Initializes the dataset from file paths or a YAML config.\n",
        "\n",
        "        Args:\n",
        "            feature_file (str): Path to the feature file (CSV/TSV).\n",
        "            label_file (str): Path to the label file (CSV/TSV).\n",
        "            sep (str): Column separator in the file. Default is tab ('\\t').\n",
        "            skiprows (int): Number of rows to skip (usually header). Default is 1.\n",
        "            config_path (str): Optional path to a YAML config file containing keys:\n",
        "                               'feature_file', 'label_file', 'sep', and 'skiprows'.\n",
        "        \"\"\"\n",
        "        if config_path:\n",
        "            with open(config_path, 'r') as config_file:\n",
        "                config = yaml.load(config_file, Loader=yaml.FullLoader)\n",
        "            feature_file = config.get('feature_file')\n",
        "            label_file = config.get('label_file')\n",
        "            sep = config.get('sep', sep)\n",
        "            skiprows = config.get('skiprows', skiprows)\n",
        "\n",
        "        # Validate file paths\n",
        "        if not feature_file or not label_file:\n",
        "            raise ValueError(\"Both feature_file and label_file must be provided either directly or through the config file.\")\n",
        "\n",
        "        # Load data\n",
        "        feature_data = pd.read_csv(feature_file, sep=sep, skiprows=skiprows)\n",
        "        label_data = pd.read_csv(label_file, sep=sep, skiprows=skiprows)\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        self.X = torch.tensor(feature_data.values, dtype=torch.float32)\n",
        "        self.Y = torch.tensor(label_data.values, dtype=torch.float32)\n",
        "        self.n_samples = self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Returns a single (feature, label) pair at the given index.\n",
        "        \"\"\"\n",
        "        return self.X[index], self.Y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of samples.\n",
        "        \"\"\"\n",
        "        return self.n_samples\n",
        "\n",
        "\n",
        "class NNModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network model with customizable hidden layers and initialization.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, initialization, dropout):\n",
        "        \"\"\"\n",
        "        Initialize the neural network model.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Number of input features.\n",
        "            hidden_sizes (list): List of hidden layer sizes.\n",
        "            output_size (int): Number of output units.\n",
        "            initialization (str): Type of weight initialization.\n",
        "            dropout (float): Dropout probability.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.input_size = input_size\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
        "        self.initialize_weights(self.fc1, initialization)\n",
        "\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        for i in range(len(hidden_sizes) - 1):\n",
        "            layer = nn.Linear(hidden_sizes[i], hidden_sizes[i + 1])\n",
        "            self.hidden_layers.append(layer)\n",
        "            self.initialize_weights(layer, initialization)\n",
        "\n",
        "        self.fc_out = nn.Linear(hidden_sizes[-1], output_size)\n",
        "        self.initialize_weights(self.fc_out, initialization)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor.\n",
        "        \"\"\"\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        for hidden_layer in self.hidden_layers:\n",
        "            x = F.relu(hidden_layer(x))\n",
        "        x = self.fc_out(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        \"\"\"\n",
        "        Compute number of features after flattening.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of flattened features.\n",
        "        \"\"\"\n",
        "        size = x.size()[1:]\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "    def initialize_weights(self, layer, initialization):\n",
        "        \"\"\"\n",
        "        Initialize weights of a given layer.\n",
        "\n",
        "        Args:\n",
        "            layer (nn.Module): Layer to initialize.\n",
        "            initialization (str): Initialization type.\n",
        "        \"\"\"\n",
        "        if initialization == 'uniform':\n",
        "            nn.init.uniform_(layer.weight, -0.1, 0.1)\n",
        "            nn.init.constant_(layer.bias, 0)\n",
        "        elif initialization == 'normal':\n",
        "            nn.init.normal_(layer.weight, mean=0, std=0.01)\n",
        "            nn.init.constant_(layer.bias, 0)\n",
        "        elif initialization == 'xavier':\n",
        "            nn.init.xavier_uniform_(layer.weight)\n",
        "            nn.init.constant_(layer.bias, 0)\n",
        "        elif initialization == 'he':\n",
        "            nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
        "            nn.init.constant_(layer.bias, 0)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid initialization type. Choose from 'uniform', 'normal', 'xavier', or 'he'.\")\n",
        "\n",
        "    def print_output_layer_weights(self, epoch):\n",
        "        \"\"\"\n",
        "        Print weights of the output layer at a given epoch.\n",
        "\n",
        "        Args:\n",
        "            epoch (int): Current training epoch.\n",
        "        \"\"\"\n",
        "        print(f\"Epoch {epoch}: fc_out\")\n",
        "        print(self.fc_out.weight.data)\n",
        "\n",
        "\n",
        "class ProportionalBatchSampler(Sampler):\n",
        "    \"\"\"\n",
        "    Custom batch sampler that ensures proportional sampling based on state labels.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_source, batch_size, state_start_idx, state_end_idx):\n",
        "        \"\"\"\n",
        "        Initialize the batch sampler.\n",
        "\n",
        "        Args:\n",
        "            data_source (torch.utils.data.TensorDataset): Dataset to sample from.\n",
        "            batch_size (int): Size of each batch.\n",
        "            state_start_idx (int): Start index for state slice.\n",
        "            state_end_idx (int): End index for state slice.\n",
        "        \"\"\"\n",
        "        self.data_source = data_source\n",
        "        self.batch_size = batch_size\n",
        "        self.state_start_idx = state_start_idx\n",
        "        self.state_end_idx = state_end_idx\n",
        "        self.num_samples = len(data_source)\n",
        "        self._prepare_indices()\n",
        "\n",
        "    def _prepare_indices(self):\n",
        "        \"\"\"Prepare internal indices and proportions for sampling.\"\"\"\n",
        "        input_tensor = self.data_source.tensors[0]\n",
        "        states_tensor = input_tensor[:, self.state_start_idx:self.state_end_idx]\n",
        "        state_indices = torch.argmax(states_tensor, dim=1).numpy()\n",
        "\n",
        "        self.state_to_indices = {}\n",
        "        for idx, state in enumerate(state_indices):\n",
        "            self.state_to_indices.setdefault(state, []).append(idx)\n",
        "\n",
        "        for indices in self.state_to_indices.values():\n",
        "            np.random.shuffle(indices)\n",
        "\n",
        "        self.state_proportions = {\n",
        "            state: len(indices) / self.num_samples\n",
        "            for state, indices in self.state_to_indices.items()\n",
        "        }\n",
        "\n",
        "        self.state_pointers = {state: 0 for state in self.state_to_indices}\n",
        "        self.num_batches = math.ceil(self.num_samples / self.batch_size)\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of indices at each iteration.\"\"\"\n",
        "        for _ in range(self.num_batches):\n",
        "            batch_indices = []\n",
        "\n",
        "            for state, proportion in self.state_proportions.items():\n",
        "                n = int(round(proportion * self.batch_size))\n",
        "                start = self.state_pointers[state]\n",
        "                end = min(start + n, len(self.state_to_indices[state]))\n",
        "                batch_indices.extend(self.state_to_indices[state][start:end])\n",
        "                self.state_pointers[state] = end\n",
        "\n",
        "            if len(batch_indices) < self.batch_size:\n",
        "                remaining = self.batch_size - len(batch_indices)\n",
        "                all_indices = sum(self.state_to_indices.values(), [])\n",
        "                np.random.shuffle(all_indices)\n",
        "                batch_indices.extend(all_indices[:remaining])\n",
        "\n",
        "            np.random.shuffle(batch_indices)\n",
        "            yield batch_indices\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of batches.\"\"\"\n",
        "        return self.num_batches\n",
        "\n",
        "\n",
        "def train_model(model, device, train_loader, optimizer, pos_weight):\n",
        "    \"\"\"\n",
        "    Train the neural network on training data.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Neural network model.\n",
        "        device (torch.device): Computation device.\n",
        "        train_loader (DataLoader): Training data loader.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer.\n",
        "        pos_weight (torch.Tensor): Weight for positive class.\n",
        "\n",
        "    Returns:\n",
        "        float: Average training loss per sample.\n",
        "    \"\"\"\n",
        "    model = model.double()\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        data, target = data.to(device).double(), target.to(device).double()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    return train_loss / len(train_loader.dataset)\n",
        "\n",
        "\n",
        "def valid_model(model, device, valid_loader, pos_weight):\n",
        "    \"\"\"\n",
        "    Evaluate the model on validation data.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Neural network model.\n",
        "        device (torch.device): Computation device.\n",
        "        valid_loader (DataLoader): Validation data loader.\n",
        "        pos_weight (torch.Tensor): Weight for positive class.\n",
        "\n",
        "    Returns:\n",
        "        float: Average validation loss per sample.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model = model.to(device).double()\n",
        "    valid_loss = 0.0\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight).to(device).double()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in valid_loader:\n",
        "            data, target = data.to(device).double(), target.to(device).double()\n",
        "            output = model(data).to(device).double()\n",
        "            loss = criterion(output, target).to(device).double()\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "    return valid_loss / len(valid_loader.dataset)\n",
        "\n",
        "\n",
        "def compute_accuracy(model, data_loader, device):\n",
        "    \"\"\"\n",
        "    Compute accuracy and classification metrics.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Neural network model.\n",
        "        data_loader (DataLoader): Data loader.\n",
        "        device (torch.device): Computation device.\n",
        "\n",
        "    Returns:\n",
        "        tuple: accuracy, precision, recall, f1, FP, FN, TP, TN, confusion matrix\n",
        "    \"\"\"\n",
        "    model = model.to(device).float()\n",
        "    model.eval()\n",
        "\n",
        "    CM = torch.zeros(2, 2, dtype=torch.int32)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = torch.tensor([], dtype=torch.float32, device=device)\n",
        "    all_targets = torch.tensor([], dtype=torch.float32, device=device)\n",
        "\n",
        "    precision = Precision(average='macro', num_classes=1, task='binary').to(device)\n",
        "    recall = Recall(average='macro', num_classes=1, task='binary').to(device)\n",
        "    f1_score = F1Score(average='macro', num_classes=1, task='binary').to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            data, target = data.to(device).float(), target.to(device).float()\n",
        "            output = model(data)\n",
        "            predicted = (torch.sigmoid(output) >= 0.5).float()\n",
        "\n",
        "            correct += (predicted == target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "            all_predictions = torch.cat((all_predictions, predicted), dim=0)\n",
        "            all_targets = torch.cat((all_targets, target), dim=0)\n",
        "\n",
        "            CM += torch.tensor(confusion_matrix(target.cpu(), predicted.cpu(), labels=[0, 1]))\n",
        "\n",
        "    accuracy = correct / total * 100\n",
        "    precision.update(all_predictions, all_targets)\n",
        "    recall.update(all_predictions, all_targets)\n",
        "    f1_score.update(all_predictions, all_targets)\n",
        "\n",
        "    return (\n",
        "        accuracy,\n",
        "        precision.compute().item(),\n",
        "        recall.compute().item(),\n",
        "        f1_score.compute().item(),\n",
        "        CM[0][1].item(),  # False Positive\n",
        "        CM[1][0].item(),  # False Negative\n",
        "        CM[1][1].item(),  # True Positive\n",
        "        CM[0][0].item(),  # True Negative\n",
        "        CM\n",
        "    )\n",
        "\n",
        "\n",
        "def compute_roc(model, data_loader, device):\n",
        "    \"\"\"\n",
        "    Compute ROC curve and AUC score.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Neural network model.\n",
        "        data_loader (DataLoader): Data loader.\n",
        "        device (torch.device): Computation device.\n",
        "\n",
        "    Returns:\n",
        "        tuple: fpr, tpr, thresholds, auc_score\n",
        "    \"\"\"\n",
        "    model = model.to(device).float()\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = torch.tensor([], dtype=torch.float32, device=device)\n",
        "    all_targets = torch.tensor([], dtype=torch.float32, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            data, target = data.to(device).float(), target.to(device).float()\n",
        "            output = model(data)\n",
        "            predicted = torch.sigmoid(output)\n",
        "\n",
        "            all_predictions = torch.cat((all_predictions, predicted), dim=0)\n",
        "            all_targets = torch.cat((all_targets, target), dim=0)\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(all_targets.cpu().numpy(), all_predictions.cpu().numpy())\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    print(f'Number of thresholds: {len(thresholds)}')\n",
        "    return fpr, tpr, thresholds, roc_auc\n",
        "\n",
        "\n",
        "def compute_prc(model, data_loader, device):\n",
        "    \"\"\"\n",
        "    Computes the Precision-Recall Curve (PRC) and AUC for a binary classification model.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): Trained model for making predictions.\n",
        "        data_loader (torch.utils.data.DataLoader): DataLoader for evaluation data.\n",
        "        device (torch.device): Device (CPU or GPU) for model and data.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Precision, recall, PRC AUC, and thresholds arrays.\n",
        "    \"\"\"\n",
        "    model = model.to(device).float()\n",
        "    model.eval()\n",
        "    \n",
        "    all_predictions = torch.tensor([], dtype=torch.float32, device=device)\n",
        "    all_targets = torch.tensor([], dtype=torch.float32, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            data, target = data.to(device).float(), target.to(device).float()\n",
        "            output = model(data)\n",
        "            predicted = torch.sigmoid(output)\n",
        "\n",
        "            all_predictions = torch.cat((all_predictions, predicted), dim=0)\n",
        "            all_targets = torch.cat((all_targets, target), dim=0)\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(all_targets.cpu().numpy(), all_predictions.cpu().numpy())\n",
        "    prc_auc = auc(recall, precision)\n",
        "    \n",
        "    return precision, recall, prc_auc, thresholds\n",
        "\n",
        "\n",
        "class MetricsLogger:\n",
        "    \"\"\"\n",
        "    A class to log and plot training and validation metrics during model training.\n",
        "\n",
        "    Attributes:\n",
        "        metrics (list): List of metrics to be tracked (e.g., loss, accuracy, precision, etc.).\n",
        "        train_{metric} (dict): Stores metric values for the training phase (indexed by epoch).\n",
        "        valid_{metric} (dict): Stores metric values for the validation phase (indexed by epoch).\n",
        "\n",
        "    Methods:\n",
        "        update(epoch, phase, **kwargs): Updates metrics for a given epoch and phase ('train' or 'valid').\n",
        "        get_metric(phase, metric): Retrieves the metric values for a specific phase and metric.\n",
        "        plot_metric(metric): Plots the specified metric across epochs for both training and validation.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Define the list of metrics and their phases\n",
        "        self.metrics = [\n",
        "            'losses', 'accuracies', 'false_positives', 'false_negatives', \n",
        "            'fpr', 'tpr', 'roc_auc', 'precision', 'recall', 'prc_auc', 'thresholds'\n",
        "        ]\n",
        "        \n",
        "        # Initialize dictionaries for each metric for both train and valid phases\n",
        "        for metric in self.metrics:\n",
        "            setattr(self, f\"train_{metric}\", {})\n",
        "            setattr(self, f\"valid_{metric}\", {})\n",
        "\n",
        "    def update(self, epoch, phase, **kwargs):\n",
        "        \"\"\"Update metrics at a given epoch and phase ('train' or 'valid')\"\"\"\n",
        "        for key, value in kwargs.items():\n",
        "            # Dynamically form the attribute name\n",
        "            attr_name = f\"{phase}_{key}\"\n",
        "            if hasattr(self, attr_name):\n",
        "                getattr(self, attr_name)[epoch] = value\n",
        "            else:\n",
        "                print(f\"Warning: Unknown metric {attr_name}\")\n",
        "\n",
        "    def get_metric(self, phase, metric):\n",
        "        \"\"\"Get a dictionary of the metric values\"\"\"\n",
        "        attr_name = f\"{phase}_{metric}\"\n",
        "        return getattr(self, attr_name, None)\n",
        "\n",
        "    def plot_metric(self, metric):\n",
        "        \"\"\"Plot a metric across epochs\"\"\"\n",
        "        train_metric = getattr(self, f\"train_{metric}\", None)\n",
        "        valid_metric = getattr(self, f\"valid_{metric}\", None)\n",
        "\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        if train_metric:\n",
        "            plt.plot(train_metric.keys(), train_metric.values(), label='Train')\n",
        "        if valid_metric:\n",
        "            plt.plot(valid_metric.keys(), valid_metric.values(), label='Validation')\n",
        "        plt.title(f'{metric.replace(\"_\", \" \").title()} over Epochs')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel(metric.replace(\"_\", \" \").title())\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744403736435
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span style=\"font-size:18px; color:#007acc;\"><b> 4. Execution code for model training   <a id=\"Execution Code for Model Training\"></a> ## "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This script is responsible for loading configuration files, setting up the dataset, \n",
        "and preparing it for training and evaluation in a machine learning pipeline using PyTorch. \n",
        "\n",
        "Key operations performed in the script:\n",
        "1. **Configuration Loading**: Loads configurations from 'config.yaml' and 'train_config.yaml' for setting random seed, device preference, \n",
        "   and model-specific parameters such as epochs, input size, and batch size.\n",
        "   \n",
        "2. **Data Preprocessing**: \n",
        "   - Loads the dataset and splits it into training, validation, and test sets using stratified splitting.\n",
        "   - Scales the dataset using a MinMaxScaler (can be extended to other scalers).\n",
        "   - Converts the data into PyTorch tensors and creates corresponding `TensorDataset` objects for training, validation, and testing.\n",
        "\n",
        "3. **Model and Training Setup**: \n",
        "   - Initializes the model, optimizer (Adam), learning rate scheduler (MultiStepLR), and loss function.\n",
        "   - Configures batch size dynamically based on the given percentage of the training dataset.\n",
        "   - Supports handling of class imbalance by adjusting positive weights based on class distribution.\n",
        "\n",
        "4. **Training Loop**: \n",
        "   - Iterates through epochs, trains the model, computes accuracy, loss, false positives, false negatives, ROC, and PRC metrics for both training \n",
        "     and validation datasets, and logs them using the `MetricsLogger` class.\n",
        "   - Updates the learning rate scheduler after each epoch.\n",
        "\n",
        "5. **Evaluation**: \n",
        "   - Computes the ROC curve and PRC metrics at each epoch for both training and validation datasets.\n",
        "   \n",
        "6. **Metric Plotting**: \n",
        "   - After training, plots the metrics (accuracy, loss, false positives, false negatives, ROC AUC, PRC AUC) for visualization using the `plot_metric` method of the `MetricsLogger`.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Load config from YAML\n",
        "with open('config.yaml', 'r') as file:\n",
        "    config = yaml.safe_load(file)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(config['random_seed'])\n",
        "torch.manual_seed(config['random_seed'])\n",
        "\n",
        "# Load dataset\n",
        "dataset = LoadDataset(config_path='config.yaml')\n",
        "\n",
        "# Split the dataset into features (X) and labels (Y)\n",
        "X, Y = dataset.X, dataset.Y\n",
        "\n",
        "# Perform initial split of the data into training and temporary sets, stratifying by Y\n",
        "X_train, X_temp, Y_train, Y_temp = train_test_split(\n",
        "    X, Y,\n",
        "    test_size=config['test_size_initial'],\n",
        "    random_state=config['random_seed'],\n",
        "    stratify=Y\n",
        ")\n",
        "\n",
        "# Further split the temporary set into validation and test sets, stratifying by Y_temp\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(\n",
        "    X_temp, Y_temp,\n",
        "    test_size=config['test_size_final'],\n",
        "    random_state=config['random_seed'],\n",
        "    stratify=Y_temp\n",
        ")\n",
        "\n",
        "# Instantiate scaler (support only MinMaxScaler here, but can be extended)\n",
        "if config['scaling'] == 'MinMaxScaler':\n",
        "    sc = MinMaxScaler()\n",
        "else:\n",
        "    raise ValueError(f\"Scaler '{config['scaling']}' is not supported.\")\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_normalized = sc.fit_transform(X_train)\n",
        "\n",
        "# Transform the validation and test data using the same scaler\n",
        "X_val_normalized = sc.transform(X_val)\n",
        "X_test_normalized = sc.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "train_input_tensor = torch.from_numpy(X_train_normalized).float()\n",
        "train_output_tensor = torch.from_numpy(Y_train).float()\n",
        "valid_input_tensor = torch.from_numpy(X_val_normalized).float()\n",
        "valid_output_tensor = torch.from_numpy(Y_val).float()\n",
        "test_input_tensor = torch.from_numpy(X_test_normalized).float()\n",
        "test_output_tensor = torch.from_numpy(Y_test).float()\n",
        "\n",
        "# PyTorch train, validation, and test sets\n",
        "train = TensorDataset(train_input_tensor, train_output_tensor)\n",
        "valid = TensorDataset(valid_input_tensor, valid_output_tensor)\n",
        "test = TensorDataset(test_input_tensor, test_output_tensor)\n",
        "\n",
        "# Initialize the metrics logger\n",
        "metrics_logger = MetricsLogger()\n",
        "\n",
        "# Set device\n",
        "if config['device_preference'] == 'auto':\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "else:\n",
        "    device = torch.device(config['device_preference'])\n",
        "print(device)\n",
        "\n",
        "##### Load training config from YAML #####\n",
        "with open('train_config.yaml', 'r') as file:\n",
        "    train_config = yaml.safe_load(file)['training']\n",
        "\n",
        "# Extract config values\n",
        "# Define the variable names you want to extract\n",
        "config_keys = [\n",
        "    'num_epochs', 'input_size', 'output_size', 'hidden_sizes', 'initialization',\n",
        "    'dropout', 'learning_rate', 'weight_decay', 'pos_weights', 'batch_percentage',\n",
        "    'scheduler.milestones', 'scheduler.gamma', 'state_start_index', 'state_end_index'\n",
        "]\n",
        "\n",
        "# Use a loop to extract and assign values dynamically\n",
        "for key in config_keys:\n",
        "    keys = key.split('.')  # Handle nested keys like scheduler.milestones\n",
        "    value = train_config\n",
        "    for k in keys:\n",
        "        value = value[k]  # Traverse through nested dictionaries\n",
        "    globals()[key.replace('.', '_')] = value  # Assign the value to a variable with the same name\n",
        "\n",
        "# Calculate batch size\n",
        "total_train_samples = len(train)\n",
        "batch_size = int(total_train_samples * batch_percentage)\n",
        "batch_size = max(1, batch_size)\n",
        "\n",
        "# DataLoaders\n",
        "train_sampler = ProportionalBatchSampler(train, batch_size=batch_size, state_start_idx, state_end_idx)\n",
        "train_loader = DataLoader(train, batch_sampler=train_sampler)\n",
        "valid_loader = DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Iterate through positive weights\n",
        "for pos_weight_mul in pos_weights:\n",
        "    pos_count = torch.sum(train_output_tensor == 1).item()\n",
        "    neg_count = torch.sum(train_output_tensor == 0).item()\n",
        "\n",
        "    if pos_count == 0:\n",
        "        raise ValueError(\"No positive samples in the dataset, cannot compute pos_weight.\")\n",
        "\n",
        "    pos_weight = (neg_count / pos_count) * pos_weight_mul\n",
        "    pos_weight = torch.tensor([pos_weight], device=device)\n",
        "\n",
        "    # Initialize model\n",
        "    model = NNModel(input_size, hidden_sizes, output_size, initialization, dropout).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=scheduler_milestones, gamma=scheduler_gamma)\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_loss = train_model(model, device, train_loader, optimizer, pos_weight)\n",
        "        metrics_logger.update(epoch, \"train\", loss=train_loss)\n",
        "\n",
        "        train_accuracy, _, _, _, train_fp, train_fn, _, _, _ = compute_accuracy(model, train_loader, device)\n",
        "        metrics_logger.update(epoch, \"train\", accuracy=train_accuracy, false_positives=train_fp, false_negatives=train_fn)\n",
        "\n",
        "        valid_loss = valid_model(model, device, valid_loader, pos_weight)\n",
        "        metrics_logger.update(epoch, \"valid\", loss=valid_loss)\n",
        "\n",
        "        valid_accuracy, _, _, _, valid_fp, valid_fn, _, _, _ = compute_accuracy(model, valid_loader, device)\n",
        "        metrics_logger.update(epoch, \"valid\", accuracy=valid_accuracy, false_positives=valid_fp, false_negatives=valid_fn)\n",
        "\n",
        "        # Scheduler step\n",
        "        scheduler.step()\n",
        "\n",
        "    # Compute ROC curve\n",
        "    train_fpr, train_tpr, train_roc_auc, train_thresholds = compute_roc(model, train_loader, device)\n",
        "    valid_fpr, valid_tpr, valid_roc_auc, valid_thresholds = compute_roc(model, valid_loader, device)\n",
        "    metrics_logger.update(epoch, \"train\", fpr=train_fpr, tpr=train_tpr, roc_auc=train_roc_auc, thresholds=train_thresholds)\n",
        "    metrics_logger.update(epoch, \"valid\", fpr=valid_fpr, tpr=valid_tpr, roc_auc=valid_roc_auc, thresholds=valid_thresholds)\n",
        "\n",
        "    # Compute PRC\n",
        "    train_precision, train_recall, train_prc_auc, train_thresholds = compute_prc(model, train_loader, device)\n",
        "    valid_precision, valid_recall, valid_prc_auc, valid_thresholds = compute_prc(model, valid_loader, device)\n",
        "    metrics_logger.update(epoch, \"train\", precision=train_precision, recall=train_recall, prc_auc=train_prc_auc, thresholds=train_thresholds)\n",
        "    metrics_logger.update(epoch, \"valid\", precision=valid_precision, recall=valid_recall, prc_auc=valid_prc_auc, thresholds=valid_thresholds)\n",
        "\n",
        "# Plot the metrics\n",
        "metrics_logger.plot_metric(\"accuracy\")\n",
        "metrics_logger.plot_metric(\"loss\")\n",
        "metrics_logger.plot_metric(\"false_positives\")\n",
        "metrics_logger.plot_metric(\"false_negatives\")\n",
        "metrics_logger.plot_metric(\"roc_auc\")\n",
        "metrics_logger.plot_metric(\"prc_auc\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744403490687
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span style=\"font-size:18px; color:#007acc;\"><b> 5. Shap analysis   <a id=\"Shap analysis\"></a> ## "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "Performs SHAP (Shapley Additive Explanations) analysis on a machine learning model to \n",
        "interpret feature contributions to predictions. \n",
        "\n",
        "1. Computes SHAP values using SHAP's DeepExplainer.\n",
        "2. Visualizes feature importance with a summary plot and a bar plot.\n",
        "\n",
        "Requires a trained model and input data (`valid_input_tensor` and `train_input_tensor`).\n",
        "\"\"\"\n",
        "\n",
        "def shap_analysis(model, input_tensor):\n",
        "    \"\"\"Performs SHAP analysis using the DeepExplainer.\"\"\"\n",
        "    # Ensure model is in evaluation mode and on CPU\n",
        "    model.eval()\n",
        "    model = model.cpu()\n",
        "\n",
        "    # Create SHAP explainer\n",
        "    explainer = shap.DeepExplainer(model, input_tensor)\n",
        "\n",
        "    # Compute SHAP values\n",
        "    shap_values = explainer.shap_values(input_tensor, check_additivity=False)\n",
        "    \n",
        "    return shap_values\n",
        "\n",
        "\n",
        "# Perform SHAP analysis\n",
        "subset_tensor = valid_input_tensor[:10000]\n",
        "model.eval()\n",
        "model = model.cpu()\n",
        "model_predictions = model(subset_tensor).detach().numpy()[:, 0]\n",
        "\n",
        "shap_values = shap_analysis(model, subset_tensor)\n",
        "shap_values = np.squeeze(shap_values)  # This will remove all dimensions of size 1\n",
        "\n",
        "shap_values = np.array(shap_values)\n",
        "\n",
        "# Sum over the feature dimension (axis=1)\n",
        "instance_sum = np.sum(shap_values, axis=1)\n",
        "\n",
        "# Generate the summary plot\n",
        "shap.summary_plot(\n",
        "    shap_values, \n",
        "    features=subset_tensor.cpu().numpy(), \n",
        "    feature_names=[\"Feature_\" + str(i) for i in range(train_input_tensor.size(1))]\n",
        ")\n",
        "\n",
        "# Generate the bar plot\n",
        "shap.plots.bar(shap_exp)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span style=\"font-size:18px; color:#007acc;\"><b> 6. Creating FAST API <a id=\"Create FAST API\"></a> ##    "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This script serves a trained machine learning model as an API using FastAPI. The model is loaded from a configuration \n",
        "file, and the weights are loaded from a pre-trained model file. It accepts input data through HTTP POST requests, \n",
        "makes predictions using the loaded model, and returns the results.\n",
        "\n",
        "Functions:\n",
        "- load_model_config: Loads the model configuration (training parameters) from a YAML configuration file.\n",
        "- run_app: Runs the FastAPI server in a separate thread to serve predictions.\n",
        "- predict: A POST endpoint that takes input features, processes them, and returns the model's prediction.\n",
        "\n",
        "Classes:\n",
        "- InputData: A Pydantic model for validating the structure of the incoming data.\n",
        "\n",
        "The FastAPI server listens on port 8000 and provides a `/predict/` endpoint for model inference. The server is run in \n",
        "a separate thread to allow asynchronous execution of requests.\n",
        "\n",
        "Requirements:\n",
        "- FastAPI for serving the model as an API.\n",
        "- PyTorch for loading the model and making predictions.\n",
        "- YAML for loading configuration from the 'config' file.\n",
        "\"\"\"\n",
        "\n",
        "# Load model configuration from the config file\n",
        "def load_model_config():\n",
        "    with open('config', 'r') as file:  # Load 'config' file\n",
        "        config = yaml.safe_load(file)\n",
        "    return config['training']\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Define a Pydantic model for input data\n",
        "class InputData(BaseModel):\n",
        "    features: list\n",
        "\n",
        "# Load model parameters from the config file\n",
        "model_config = load_model_config()\n",
        "\n",
        "input_size = model_config['input_size']\n",
        "output_size = model_config['output_size']\n",
        "hidden_sizes = model_config['hidden_sizes']\n",
        "initialization = model_config['initialization']\n",
        "\n",
        "# Create the model instance\n",
        "model = NNModel(input_size, hidden_sizes, output_size, initialization)\n",
        "model.load_state_dict(torch.load('model_weights.pth'))  # Load pre-trained weights\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "@app.post(\"/predict/\")\n",
        "async def predict(data: InputData):\n",
        "    # Convert input data to tensor and ensure it has the correct shape\n",
        "    input_tensor = torch.tensor(data.features, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
        "    with torch.no_grad():\n",
        "        prediction = model(input_tensor)\n",
        "    return {\"prediction\": prediction.tolist()}\n",
        "\n",
        "# Function to run the app in a separate thread\n",
        "def run_app():\n",
        "    run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "\n",
        "# Start the FastAPI server in a separate thread\n",
        "thread = threading.Thread(target=run_app)\n",
        "thread.start()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span style=\"font-size:18px; color:#007acc;\"><b> 7. Inference from API <a id=\"Inference from API\"></a> ##    "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This script extracts a single example from the test dataset, sends it to a FastAPI model server for prediction, and \n",
        "prints both the predicted and true labels. The steps include:\n",
        "\n",
        "1. Extract the first example (feature set) from the input tensor and the corresponding true label.\n",
        "2. Send a POST request to the FastAPI endpoint with the feature data.\n",
        "3. Process the response, which is assumed to contain logits from the model, apply the sigmoid activation function \n",
        "   to get probabilities, and convert them into binary predictions (0 or 1).\n",
        "4. Print the status code and response from the server, as well as the predicted labels and true label for comparison.\n",
        "\n",
        "Error handling is included for failed requests and invalid JSON responses.\n",
        "\n",
        "Dependencies:\n",
        "- requests for making HTTP requests.\n",
        "- The sigmoid function to convert logits into probabilities.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Extract the first example (row) from test_input_tensor and convert it to a list\n",
        "features = test_input_tensor[:1].tolist()[0]  # Extracts the first example and converts it to a list\n",
        "\n",
        "# Extract the true label corresponding to the first example\n",
        "true_label = test_output_tensor[0].item()  # Converts the tensor value to a Python scalar\n",
        "\n",
        "# Define the endpoint\n",
        "url = \"http://localhost:8000/predict/\"\n",
        "\n",
        "data = {\"features\": features}  # Use the extracted features\n",
        "\n",
        "try:\n",
        "    # Make the POST request\n",
        "    response = requests.post(url, json=data)\n",
        "\n",
        "    # Print status code\n",
        "    print(f\"Status Code: {response.status_code}\")\n",
        "\n",
        "    # Print response text for debugging\n",
        "    print(f\"Response Text: {response.text}\")\n",
        "\n",
        "    # Assuming the response is a JSON array of logits\n",
        "    logits = response.json()\n",
        "\n",
        "    # Apply the sigmoid function and threshold to each logit\n",
        "    prediction = sigmoid(logits)\n",
        "    prediction = [1 if logit >= 0.5 else 0 for logit in logits]\n",
        "\n",
        "    # Print the resulting predictions (0 or 1)\n",
        "    print(\"Predicted Labels:\", prediction)\n",
        "\n",
        "    # Print the true label\n",
        "    print(\"True Label:\", true_label)\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Request failed: {e}\")\n",
        "except ValueError as e:\n",
        "    print(f\"JSON decode error: {e}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}