{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“Œ <span style=\"font-size:18px; color:#007acc;\"><b>Introduction</b></span>\n",
        "\n",
        "This is the second notebook in the series. The first notebook focused on data preprocessing and cleaning, including handling missing values, detecting and removing outliers, and performing exploratory data analysis (EDA). In this notebook, we continue from that point by importing necessary modules and defining custom functions and classes for model training and testing. A custom batch sampler is implemented for efficient data loading. For interpretability, SHAP analysis is performed to explain feature importance. The trained model is then saved for future use, and a FastAPI server is created to serve the model for local inference. Finally, predictions can be made by sending requests to the API. Note: This work was conducted on MS Azure, so some settings may need adjustments.\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span style=\"font-size:18px; color:#007acc;\"><b> Table of Contents</b></span>\n",
        "1. [Install libraries](#Install-Libraries)\n",
        "2. [Import modules](#Import-Modules) \n",
        "3. [Custom functions and classes for model training](#Custom-Functions-and-Classes-for-Model-Training)  \n",
        "4. [Execution code for model training](#Execution-Code-for-Model-Training)  \n",
        "5. [SHAP analysis](#SHAP-Analysis)\n",
        "6. [Creating FAST API](#Create-FAST-API)\n",
        "7. [Inference from API](#Inference-from-API)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span style=\"font-size:18px; color:#007acc;\"><b> 1. Install libraries <a id=\"Install libraries\"></a> ##"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required libraries to be installed\n",
        "!pip install numpy \n",
        "!pip install pandas\n",
        "!pip install seaborn\n",
        "!pip install matplotlib\n",
        "!pip install azureml-core  # AzureML Core for interacting with Azure Machine Learning services\n",
        "!pip install azure-storage-blob  # Azure Storage Blob for managing files in Azure Blob Storage\n",
        "!pip install azureml-dataset-runtime  # AzureML Dataset Runtime for working with datasets in Azure ML\n",
        "!pip install torch torchvision torchmetrics  # PyTorch for deep learning, torchvision for computer vision, torchmetrics for metrics\n",
        "!pip install scikit-learn pandas numpy matplotlib seaborn\n",
        "!pip install pyyaml\n",
        "!pip install mlflow  # MLflow for managing machine learning workflows\n",
        "!pip install fastapi uvicorn  # FastAPI for building web APIs\n",
        "!pip install fsspec  # fsspec for working with file systems"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744403735907
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span style=\"font-size:18px; color:#007acc;\"><b> 2. Import modules <a id=\"Import modules\"></a> ##"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard Library\n",
        "import sys\n",
        "import math\n",
        "import logging\n",
        "import datetime\n",
        "import threading\n",
        "import requests\n",
        "import yaml\n",
        "\n",
        "# PyTorch and Related\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler, random_split, TensorDataset\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchmetrics import Precision, Recall, F1Score\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve\n",
        "\n",
        "# Plotting and Visualization\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Data Handling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# API\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from uvicorn import run\n",
        "\n",
        "# Remote File Access\n",
        "import fsspec\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744403736225
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span style=\"font-size:18px; color:#007acc;\"><b> 3. Custom functions and classes for model training <a id=\"Custom functions and classes for model training\"></a> ##"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "This code defines a custom PyTorch dataset, neural network model, and several utility functions for training, validation, and evaluation of a binary classification model.\n",
        "\n",
        "1. `LoadDataset`: A custom dataset class to load tabular data from feature and label files, with support for YAML-based configuration.\n",
        "2. `NNModel`: A neural network model with customizable layers, weight initialization, and dropout.\n",
        "3. `ProportionalBatchSampler`: A custom batch sampler to ensure proportional sampling based on state labels.\n",
        "4. Functions for training, validation, accuracy computation, ROC curve computation, and precision-recall curve computation are also defined.\n",
        "\n",
        "Each class and function is documented with arguments, return types, and functionality explained.\n",
        "\n",
        "PEP 8 recommendations have been followed, with the exception of line length exceeding 79 characters in some instances. \n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class LoadDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for loading tabular data from feature and label files,\n",
        "    optionally using a YAML config for cleaner initialization.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feature_file=None, label_file=None, sep='\\t', skiprows=1, config_path=None):\n",
        "        \"\"\"\n",
        "        Initializes the dataset from file paths or a YAML config.\n",
        "\n",
        "        Args:\n",
        "            feature_file (str): Path to the feature file (CSV/TSV).\n",
        "            label_file (str): Path to the label file (CSV/TSV).\n",
        "            sep (str): Column separator in the file. Default is tab ('\\t').\n",
        "            skiprows (int): Number of rows to skip (usually header). Default is 1.\n",
        "            config_path (str): Optional path to a YAML config file containing keys:\n",
        "                               'feature_file', 'label_file', 'sep', and 'skiprows'.\n",
        "        \"\"\"\n",
        "        if config_path:\n",
        "            with open(config_path, 'r') as config_file:\n",
        "                config = yaml.load(config_file, Loader=yaml.FullLoader)\n",
        "            feature_file = config.get('feature_file')\n",
        "            label_file = config.get('label_file')\n",
        "            sep = config.get('sep', sep)\n",
        "            skiprows = config.get('skiprows', skiprows)\n",
        "\n",
        "        # Validate file paths\n",
        "        if not feature_file or not label_file:\n",
        "            raise ValueError(\"Both feature_file and label_file must be provided either directly or through the config file.\")\n",
        "\n",
        "        # Load data\n",
        "        feature_data = pd.read_csv(feature_file, sep=sep, skiprows=skiprows)\n",
        "        label_data = pd.read_csv(label_file, sep=sep, skiprows=skiprows)\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        self.X = torch.tensor(feature_data.values, dtype=torch.float32)\n",
        "        self.Y = torch.tensor(label_data.values, dtype=torch.float32)\n",
        "        self.n_samples = self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Returns a single (feature, label) pair at the given index.\n",
        "        \"\"\"\n",
        "        return self.X[index], self.Y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of samples.\n",
        "        \"\"\"\n",
        "        return self.n_samples\n",
        "\n",
        "\n",
        "class NNModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network model with customizable hidden layers and initialization.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, initialization, dropout):\n",
        "        \"\"\"\n",
        "        Initialize the neural network model.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Number of input features.\n",
        "            hidden_sizes (list): List of hidden layer sizes.\n",
        "            output_size (int): Number of output units.\n",
        "            initialization (str): Type of weight initialization.\n",
        "            dropout (float): Dropout probability.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.input_size = input_size\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
        "        self.initialize_weights(self.fc1, initialization)\n",
        "\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        for i in range(len(hidden_sizes) - 1):\n",
        "            layer = nn.Linear(hidden_sizes[i], hidden_sizes[i + 1])\n",
        "            self.hidden_layers.append(layer)\n",
        "            self.initialize_weights(layer, initialization)\n",
        "\n",
        "        self.fc_out = nn.Linear(hidden_sizes[-1], output_size)\n",
        "        self.initialize_weights(self.fc_out, initialization)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor.\n",
        "        \"\"\"\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        for hidden_layer in self.hidden_layers:\n",
        "            x = F.relu(hidden_layer(x))\n",
        "        x = self.fc_out(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        \"\"\"\n",
        "        Compute number of features after flattening.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of flattened features.\n",
        "        \"\"\"\n",
        "        size = x.size()[1:]\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "    def initialize_weights(self, layer, initialization):\n",
        "        \"\"\"\n",
        "        Initialize weights of a given layer.\n",
        "\n",
        "        Args:\n",
        "            layer (nn.Module): Layer to initialize.\n",
        "            initialization (str): Initialization type.\n",
        "        \"\"\"\n",
        "        if initialization == 'uniform':\n",
        "            nn.init.uniform_(layer.weight, -0.1, 0.1)\n",
        "            nn.init.constant_(layer.bias, 0)\n",
        "        elif initialization == 'normal':\n",
        "            nn.init.normal_(layer.weight, mean=0, std=0.01)\n",
        "            nn.init.constant_(layer.bias, 0)\n",
        "        elif initialization == 'xavier':\n",
        "            nn.init.xavier_uniform_(layer.weight)\n",
        "            nn.init.constant_(layer.bias, 0)\n",
        "        elif initialization == 'he':\n",
        "            nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
        "            nn.init.constant_(layer.bias, 0)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid initialization type. Choose from 'uniform', 'normal', 'xavier', or 'he'.\")\n",
        "\n",
        "    def print_output_layer_weights(self, epoch):\n",
        "        \"\"\"\n",
        "        Print weights of the output layer at a given epoch.\n",
        "\n",
        "        Args:\n",
        "            epoch (int): Current training epoch.\n",
        "        \"\"\"\n",
        "        print(f\"Epoch {epoch}: fc_out\")\n",
        "        print(self.fc_out.weight.data)\n",
        "\n",
        "\n",
        "class ProportionalBatchSampler(Sampler):\n",
        "    \"\"\"\n",
        "    Custom batch sampler that ensures proportional sampling based on state labels.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_source, batch_size, state_start_idx, state_end_idx):\n",
        "        \"\"\"\n",
        "        Initialize the batch sampler.\n",
        "\n",
        "        Args:\n",
        "            data_source (torch.utils.data.TensorDataset): Dataset to sample from.\n",
        "            batch_size (int): Size of each batch.\n",
        "            state_start_idx (int): Start index for state slice.\n",
        "            state_end_idx (int): End index for state slice.\n",
        "        \"\"\"\n",
        "        self.data_source = data_source\n",
        "        self.batch_size = batch_size\n",
        "        self.state_start_idx = state_start_idx\n",
        "        self.state_end_idx = state_end_idx\n",
        "        self.num_samples = len(data_source)\n",
        "        self._prepare_indices()\n",
        "\n",
        "    def _prepare_indices(self):\n",
        "        \"\"\"Prepare internal indices and proportions for sampling.\"\"\"\n",
        "        input_tensor = self.data_source.tensors[0]\n",
        "        states_tensor = input_tensor[:, self.state_start_idx:self.state_end_idx]\n",
        "        state_indices = torch.argmax(states_tensor, dim=1).numpy()\n",
        "\n",
        "        self.state_to_indices = {}\n",
        "        for idx, state in enumerate(state_indices):\n",
        "            self.state_to_indices.setdefault(state, []).append(idx)\n",
        "\n",
        "        for indices in self.state_to_indices.values():\n",
        "            np.random.shuffle(indices)\n",
        "\n",
        "        self.state_proportions = {\n",
        "            state: len(indices) / self.num_samples\n",
        "            for state, indices in self.state_to_indices.items()\n",
        "        }\n",
        "\n",
        "        self.state_pointers = {state: 0 for state in self.state_to_indices}\n",
        "        self.num_batches = math.ceil(self.num_samples / self.batch_size)\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of indices at each iteration.\"\"\"\n",
        "        for _ in range(self.num_batches):\n",
        "            batch_indices = []\n",
        "\n",
        "            for state, proportion in self.state_proportions.items():\n",
        "                n = int(round(proportion * self.batch_size))\n",
        "                start = self.state_pointers[state]\n",
        "                end = min(start + n, len(self.state_to_indices[state]))\n",
        "                batch_indices.extend(self.state_to_indices[state][start:end])\n",
        "                self.state_pointers[state] = end\n",
        "\n",
        "            if len(batch_indices) < self.batch_size:\n",
        "                remaining = self.batch_size - len(batch_indices)\n",
        "                all_indices = sum(self.state_to_indices.values(), [])\n",
        "                np.random.shuffle(all_indices)\n",
        "                batch_indices.extend(all_indices[:remaining])\n",
        "\n",
        "            np.random.shuffle(batch_indices)\n",
        "            yield batch_indices\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of batches.\"\"\"\n",
        "        return self.num_batches\n",
        "\n",
        "\n",
        "def train_model(model, device, train_loader, optimizer, pos_weight):\n",
        "    \"\"\"\n",
        "    Train the neural network on training data.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Neural network model.\n",
        "        device (torch.device): Computation device.\n",
        "        train_loader (DataLoader): Training data loader.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer.\n",
        "        pos_weight (torch.Tensor): Weight for positive class.\n",
        "\n",
        "    Returns:\n",
        "        float: Average training loss per sample.\n",
        "    \"\"\"\n",
        "    model = model.double()\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        data, target = data.to(device).double(), target.to(device).double()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    return train_loss / len(train_loader.dataset)\n",
        "\n",
        "\n",
        "def valid_model(model, device, valid_loader, pos_weight):\n",
        "    \"\"\"\n",
        "    Evaluate the model on validation data.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Neural network model.\n",
        "        device (torch.device): Computation device.\n",
        "        valid_loader (DataLoader): Validation data loader.\n",
        "        pos_weight (torch.Tensor): Weight for positive class.\n",
        "\n",
        "    Returns:\n",
        "        float: Average validation loss per sample.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model = model.to(device).double()\n",
        "    valid_loss = 0.0\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight).to(device).double()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in valid_loader:\n",
        "            data, target = data.to(device).double(), target.to(device).double()\n",
        "            output = model(data).to(device).double()\n",
        "            loss = criterion(output, target).to(device).double()\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "    return valid_loss / len(valid_loader.dataset)\n",
        "\n",
        "\n",
        "def compute_accuracy(model, data_loader, device):\n",
        "    \"\"\"\n",
        "    Compute accuracy and classification metrics.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Neural network model.\n",
        "        data_loader (DataLoader): Data loader.\n",
        "        device (torch.device): Computation device.\n",
        "\n",
        "    Returns:\n",
        "        tuple: accuracy, precision, recall, f1, FP, FN, TP, TN, confusion matrix\n",
        "    \"\"\"\n",
        "    model = model.to(device).float()\n",
        "    model.eval()\n",
        "\n",
        "    CM = torch.zeros(2, 2, dtype=torch.int32)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = torch.tensor([], dtype=torch.float32, device=device)\n",
        "    all_targets = torch.tensor([], dtype=torch.float32, device=device)\n",
        "\n",
        "    precision = Precision(average='macro', num_classes=1, task='binary').to(device)\n",
        "    recall = Recall(average='macro', num_classes=1, task='binary').to(device)\n",
        "    f1_score = F1Score(average='macro', num_classes=1, task='binary').to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            data, target = data.to(device).float(), target.to(device).float()\n",
        "            output = model(data)\n",
        "            predicted = (torch.sigmoid(output) >= 0.5).float()\n",
        "\n",
        "            correct += (predicted == target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "            all_predictions = torch.cat((all_predictions, predicted), dim=0)\n",
        "            all_targets = torch.cat((all_targets, target), dim=0)\n",
        "\n",
        "            CM += torch.tensor(confusion_matrix(target.cpu(), predicted.cpu(), labels=[0, 1]))\n",
        "\n",
        "    accuracy = correct / total * 100\n",
        "    precision.update(all_predictions, all_targets)\n",
        "    recall.update(all_predictions, all_targets)\n",
        "    f1_score.update(all_predictions, all_targets)\n",
        "\n",
        "    return (\n",
        "        accuracy,\n",
        "        precision.compute().item(),\n",
        "        recall.compute().item(),\n",
        "        f1_score.compute().item(),\n",
        "        CM[0][1].item(),  # False Positive\n",
        "        CM[1][0].item(),  # False Negative\n",
        "        CM[1][1].item(),  # True Positive\n",
        "        CM[0][0].item(),  # True Negative\n",
        "        CM\n",
        "    )\n",
        "\n",
        "\n",
        "def compute_roc(model, data_loader, device):\n",
        "    \"\"\"\n",
        "    Compute ROC curve and AUC score.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Neural network model.\n",
        "        data_loader (DataLoader): Data loader.\n",
        "        device (torch.device): Computation device.\n",
        "\n",
        "    Returns:\n",
        "        tuple: fpr, tpr, thresholds, auc_score\n",
        "    \"\"\"\n",
        "    model = model.to(device).float()\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = torch.tensor([], dtype=torch.float32, device=device)\n",
        "    all_targets = torch.tensor([], dtype=torch.float32, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            data, target = data.to(device).float(), target.to(device).float()\n",
        "            output = model(data)\n",
        "            predicted = torch.sigmoid(output)\n",
        "\n",
        "            all_predictions = torch.cat((all_predictions, predicted), dim=0)\n",
        "            all_targets = torch.cat((all_targets, target), dim=0)\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(all_targets.cpu().numpy(), all_predictions.cpu().numpy())\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    print(f'Number of thresholds: {len(thresholds)}')\n",
        "    return fpr, tpr, thresholds, roc_auc\n",
        "\n",
        "\n",
        "def compute_prc(model, data_loader, device):\n",
        "    model = model.to(device).float()\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    \n",
        "    all_predictions = torch.tensor([], dtype=torch.float32, device=device)\n",
        "    all_targets = torch.tensor([], dtype=torch.float32, device=device)\n",
        "\n",
        "    with torch.no_grad():  # No need to compute gradients during inference\n",
        "        for data, target in data_loader:\n",
        "            data, target = data.to(device).float(), target.to(device).float()\n",
        "            output = model(data)\n",
        "            predicted = torch.sigmoid(output)\n",
        "\n",
        "            all_predictions = torch.cat((all_predictions, predicted), dim=0)\n",
        "            all_targets = torch.cat((all_targets, target), dim=0)\n",
        "\n",
        "    # Convert predictions and targets to CPU and numpy arrays\n",
        "    precision, recall, thresholds = precision_recall_curve(all_targets.cpu().numpy(), all_predictions.cpu().numpy())\n",
        "    prc_auc = auc(recall, precision)\n",
        "    \n",
        "    return precision, recall, prc_auc, thresholds\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744403736435
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span style=\"font-size:18px; color:#007acc;\"><b> 4. Execution code for model training   <a id=\"Execution Code for Model Training\"></a> ## "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This script performs the following tasks:\n",
        "1. Loads configuration settings from YAML files.\n",
        "2. Instantiates the dataset and preprocesses it (scaling and splitting).\n",
        "3. Defines and trains a neural network model with specified configurations.\n",
        "4. Calculates evaluation metrics (accuracy, loss, false positives/negatives, ROC/PR curves).\n",
        "5. Saves the trained model's weights.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import yaml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch import optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load config from YAML\n",
        "with open('config.yaml', 'r') as file:\n",
        "    config = yaml.safe_load(file)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(config['random_seed'])\n",
        "torch.manual_seed(config['random_seed'])\n",
        "\n",
        "# Load dataset\n",
        "dataset = LoadDataset(config_path='config.yaml')\n",
        "\n",
        "# Split the dataset into features (X) and labels (Y)\n",
        "X, Y = dataset.X, dataset.Y\n",
        "\n",
        "# Perform initial split of the data into training and temporary sets, stratifying by Y\n",
        "X_train, X_temp, Y_train, Y_temp = train_test_split(\n",
        "    X, Y,\n",
        "    test_size=config['test_size_initial'],\n",
        "    random_state=config['random_seed'],\n",
        "    stratify=Y\n",
        ")\n",
        "\n",
        "# Further split the temporary set into validation and test sets, stratifying by Y_temp\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(\n",
        "    X_temp, Y_temp,\n",
        "    test_size=config['test_size_final'],\n",
        "    random_state=config['random_seed'],\n",
        "    stratify=Y_temp\n",
        ")\n",
        "\n",
        "# Instantiate scaler (support only MinMaxScaler here, but can be extended)\n",
        "if config['scaling'] == 'MinMaxScaler':\n",
        "    sc = MinMaxScaler()\n",
        "else:\n",
        "    raise ValueError(f\"Scaler '{config['scaling']}' is not supported.\")\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_normalized = sc.fit_transform(X_train)\n",
        "\n",
        "# Transform the validation and test data using the same scaler\n",
        "X_val_normalized = sc.transform(X_val)\n",
        "X_test_normalized = sc.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "train_input_tensor = torch.from_numpy(X_train_normalized).float()\n",
        "train_output_tensor = torch.from_numpy(Y_train).float()\n",
        "valid_input_tensor = torch.from_numpy(X_val_normalized).float()\n",
        "valid_output_tensor = torch.from_numpy(Y_val).float()\n",
        "test_input_tensor = torch.from_numpy(X_test_normalized).float()\n",
        "test_output_tensor = torch.from_numpy(Y_test).float()\n",
        "\n",
        "# PyTorch train, validation, and test sets\n",
        "train = TensorDataset(train_input_tensor, train_output_tensor)\n",
        "valid = TensorDataset(valid_input_tensor, valid_output_tensor)\n",
        "test = TensorDataset(test_input_tensor, test_output_tensor)\n",
        "\n",
        "# Class weights calculation for handling class imbalance\n",
        "train_num_positives = torch.sum(train_output_tensor == 1)\n",
        "train_num_negatives = torch.sum(train_output_tensor == 0)\n",
        "print(\"train_num_positives\", train_num_positives)\n",
        "print(\"train_num_negatives\", train_num_negatives)\n",
        "\n",
        "valid_num_positives = torch.sum(valid_output_tensor == 1)\n",
        "valid_num_negatives = torch.sum(valid_output_tensor == 0)\n",
        "print(\"valid_num_positives\", valid_num_positives)\n",
        "print(\"valid_num_negatives\", valid_num_negatives)\n",
        "\n",
        "# Set device\n",
        "if config['device_preference'] == 'auto':\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "else:\n",
        "    device = torch.device(config['device_preference'])\n",
        "print(device)\n",
        "\n",
        "##### Load training config from YAML #####\n",
        "with open('train_config.yaml', 'r') as file:\n",
        "    train_config = yaml.safe_load(file)['training']\n",
        "\n",
        "# Extract config values\n",
        "num_epochs = train_config['num_epochs']\n",
        "input_size = train_config['input_size']\n",
        "output_size = train_config['output_size']\n",
        "hidden_sizes = train_config['hidden_sizes']\n",
        "initialization = train_config['initialization']\n",
        "dropout = train_config['dropout']\n",
        "learning_rate = train_config['learning_rate']\n",
        "weight_decay = train_config['weight_decay']\n",
        "pos_weights = train_config['pos_weights']\n",
        "batch_percentage = train_config['batch_percentage']\n",
        "scheduler_milestones = train_config['scheduler']['milestones']\n",
        "scheduler_gamma = train_config['scheduler']['gamma']\n",
        "state_start_idx = train_config['state_start_index']\n",
        "state_end_idx = train_config['state_end_index']\n",
        "\n",
        "# Calculate batch size\n",
        "total_train_samples = len(train)\n",
        "batch_size = int(total_train_samples * batch_percentage)\n",
        "batch_size = max(1, batch_size)\n",
        "\n",
        "# Initialize dictionaries to store values for plotting later\n",
        "all_training_losses = {}\n",
        "all_valid_losses = {}\n",
        "all_train_accuracies = {}\n",
        "all_valid_accuracies = {}\n",
        "all_train_false_positives = {}\n",
        "all_train_false_negatives = {}\n",
        "all_valid_false_positives = {}\n",
        "all_valid_false_negatives = {}\n",
        "all_train_fpr = {}\n",
        "all_train_tpr = {}\n",
        "all_train_roc_auc = {}\n",
        "all_valid_fpr = {}\n",
        "all_valid_tpr = {}\n",
        "all_valid_roc_auc = {}\n",
        "all_train_precision = {}\n",
        "all_train_recall = {}\n",
        "all_valid_precision = {}\n",
        "all_valid_recall = {}\n",
        "all_train_prc_auc = {}\n",
        "all_valid_prc_auc = {}\n",
        "all_train_thresholds = {}\n",
        "all_valid_thresholds = {}\n",
        "\n",
        "# DataLoaders\n",
        "train_sampler = ProportionalBatchSampler(train, batch_size=batch_size, state_start_idx, state_end_idx)\n",
        "train_loader = DataLoader(train, batch_sampler=train_sampler)\n",
        "valid_loader = DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Iterate through positive weights\n",
        "for pos_weight_mul in pos_weights:\n",
        "    pos_count = torch.sum(train_output_tensor == 1).item()\n",
        "    neg_count = torch.sum(train_output_tensor == 0).item()\n",
        "\n",
        "    if pos_count == 0:\n",
        "        raise ValueError(\"No positive samples in the dataset, cannot compute pos_weight.\")\n",
        "\n",
        "    pos_weight = (neg_count / pos_count) * pos_weight_mul\n",
        "    pos_weight = torch.tensor([pos_weight], device=device)\n",
        "\n",
        "    # Initialize lists for the current run\n",
        "    Epoch_ind = []\n",
        "    training_losses = []\n",
        "    valid_losses = []\n",
        "    train_accuracies = []\n",
        "    valid_accuracies = []\n",
        "    train_false_positives = []\n",
        "    train_false_negatives = []\n",
        "    valid_false_positives = []\n",
        "    valid_false_negatives = []\n",
        "\n",
        "    # Initialize model\n",
        "    model = NNModel(input_size, hidden_sizes, output_size, initialization, dropout).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=scheduler_milestones, gamma=scheduler_gamma)\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_loss = train_model(model, device, train_loader, optimizer, pos_weight)\n",
        "        training_losses.append(train_loss)\n",
        "\n",
        "        train_accuracy, _, _, _, train_fp, train_fn, _, _, _ = compute_accuracy(model, train_loader, device)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        train_false_positives.append(train_fp)\n",
        "        train_false_negatives.append(train_fn)\n",
        "\n",
        "        valid_loss = valid_model(model, device, valid_loader, pos_weight)\n",
        "        valid_losses.append(valid_loss)\n",
        "\n",
        "        valid_accuracy, _, _, _, valid_fp, valid_fn, _, _, _ = compute_accuracy(model, valid_loader, device)\n",
        "        valid_accuracies.append(valid_accuracy)\n",
        "        valid_false_positives.append(valid_fp)\n",
        "        valid_false_negatives.append(valid_fn)\n",
        "\n",
        "        Epoch_ind.append(epoch)\n",
        "        scheduler.step()\n",
        "        print(f\"epoch {epoch}\")\n",
        "\n",
        "\n",
        "    # Compute ROC curve\n",
        "    train_fpr, train_tpr, train_roc_auc, train_thresholds = compute_roc(model, train_loader, device)\n",
        "    valid_fpr, valid_tpr, valid_roc_auc, valid_thresholds = compute_roc(model, valid_loader, device)\n",
        "    \n",
        "    # Compute PRC\n",
        "    train_precision, train_recall, train_prc_auc, train_thresholds = compute_prc(model, train_loader, device)\n",
        "    valid_precision, valid_recall, valid_prc_auc, valid_thresholds = compute_prc(model, valid_loader, device)\n",
        "    \n",
        "     # Store results for the current positive weight\n",
        "    all_training_losses[pos_weight_mul] = training_losses\n",
        "    all_valid_losses[pos_weight_mul] = valid_losses\n",
        "    all_train_accuracies[pos_weight_mul] = train_accuracies\n",
        "    all_valid_accuracies[pos_weight_mul] = valid_accuracies\n",
        "    all_train_false_positives[pos_weight_mul] = train_false_positives\n",
        "    all_train_false_negatives[pos_weight_mul] = train_false_negatives\n",
        "    all_valid_false_positives[pos_weight_mul] = valid_false_positives\n",
        "    all_valid_false_negatives[pos_weight_mul] = valid_false_negatives\n",
        "    \n",
        "    all_train_fpr[pos_weight_mul] = train_fpr\n",
        "    all_train_tpr[pos_weight_mul] = train_tpr\n",
        "    all_train_roc_auc[pos_weight_mul] = train_roc_auc\n",
        "    all_valid_fpr[pos_weight_mul] = valid_fpr\n",
        "    all_valid_tpr[pos_weight_mul] = valid_tpr\n",
        "    all_valid_roc_auc[pos_weight_mul] = valid_roc_auc\n",
        "\n",
        "    all_train_precision[pos_weight_mul] = train_precision\n",
        "    all_train_recall[pos_weight_mul] = train_recall\n",
        "    all_valid_precision[pos_weight_mul] = valid_precision\n",
        "    all_valid_recall[pos_weight_mul] = valid_recall\n",
        "    \n",
        "    all_train_prc_auc[pos_weight_mul] = train_prc_auc\n",
        "    all_valid_prc_auc[pos_weight_mul] = valid_prc_auc\n",
        "\n",
        "    all_train_thresholds[pos_weight_mul] = train_thresholds\n",
        "    all_valid_thresholds[pos_weight_mul] = valid_thresholds\n",
        "\n",
        "# Plotting results for all positive weights\n",
        "plt.figure(figsize=(15, 25))\n",
        "\n",
        "# Plot Train False Positives vs Epochs\n",
        "plt.subplot(5, 2, 1)\n",
        "for pos_weight_mul in pos_weights:\n",
        "    plt.plot(Epoch_ind, all_train_false_positives[pos_weight_mul], label=f\"Pos Weight {pos_weight_mul}\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Train False Positives')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Train False Negatives vs Epochs\n",
        "plt.subplot(5, 2, 2)\n",
        "for pos_weight_mul in pos_weights:\n",
        "    plt.plot(Epoch_ind, all_train_false_negatives[pos_weight_mul], label=f\"Pos Weight {pos_weight_mul}\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Train False Negatives')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Valid False Positives vs Epochs\n",
        "plt.subplot(5, 2, 3)\n",
        "for pos_weight_mul in pos_weights:\n",
        "    plt.plot(Epoch_ind, all_valid_false_positives[pos_weight_mul], label=f\"Pos Weight {pos_weight_mul}\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Valid False Positives')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Valid False Negatives vs Epochs\n",
        "plt.subplot(5, 2, 4)\n",
        "for pos_weight_mul in pos_weights:\n",
        "    plt.plot(Epoch_ind, all_valid_false_negatives[pos_weight_mul], label=f\"Pos Weight {pos_weight_mul}\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Valid False Negatives')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Training and Validation Loss vs Epochs\n",
        "plt.subplot(5, 2, 5)\n",
        "for pos_weight_mul in pos_weights:\n",
        "    plt.plot(Epoch_ind, all_training_losses[pos_weight_mul], label=f\"Train Loss (Pos Weight {pos_weight_mul})\")\n",
        "    plt.plot(Epoch_ind, all_training_losses[pos_weight_mul], label=f\"Validation Loss (Pos Weight {pos_weight_mul})\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Loss')\n",
        "plt.legend()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1744403490687
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span style=\"font-size:18px; color:#007acc;\"><b> 5. Shap analysis   <a id=\"Shap analysis\"></a> ## "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "Performs SHAP (Shapley Additive Explanations) analysis on a machine learning model to \n",
        "interpret feature contributions to predictions. \n",
        "\n",
        "1. Computes SHAP values using SHAP's DeepExplainer.\n",
        "2. Visualizes feature importance with a summary plot and a bar plot.\n",
        "\n",
        "Requires a trained model and input data (`valid_input_tensor` and `train_input_tensor`).\n",
        "\"\"\"\n",
        "\n",
        "def shap_analysis(model, input_tensor):\n",
        "    \"\"\"Performs SHAP analysis using the DeepExplainer.\"\"\"\n",
        "    # Ensure model is in evaluation mode and on CPU\n",
        "    model.eval()\n",
        "    model = model.cpu()\n",
        "\n",
        "    # Create SHAP explainer\n",
        "    explainer = shap.DeepExplainer(model, input_tensor)\n",
        "\n",
        "    # Compute SHAP values\n",
        "    shap_values = explainer.shap_values(input_tensor, check_additivity=False)\n",
        "    \n",
        "    return shap_values\n",
        "\n",
        "\n",
        "# Perform SHAP analysis\n",
        "subset_tensor = valid_input_tensor[:10000]\n",
        "model.eval()\n",
        "model = model.cpu()\n",
        "model_predictions = model(subset_tensor).detach().numpy()[:, 0]\n",
        "\n",
        "shap_values = shap_analysis(model, subset_tensor)\n",
        "shap_values = np.squeeze(shap_values)  # This will remove all dimensions of size 1\n",
        "\n",
        "shap_values = np.array(shap_values)\n",
        "\n",
        "# Sum over the feature dimension (axis=1)\n",
        "instance_sum = np.sum(shap_values, axis=1)\n",
        "\n",
        "# Generate the summary plot\n",
        "shap.summary_plot(\n",
        "    shap_values, \n",
        "    features=subset_tensor.cpu().numpy(), \n",
        "    feature_names=[\"Feature_\" + str(i) for i in range(train_input_tensor.size(1))]\n",
        ")\n",
        "\n",
        "# Generate the bar plot\n",
        "shap.plots.bar(shap_exp)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span style=\"font-size:18px; color:#007acc;\"><b> 6. Creating FAST API <a id=\"Create FAST API\"></a> ##    "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This script serves a trained machine learning model as an API using FastAPI. The model is loaded from a configuration \n",
        "file, and the weights are loaded from a pre-trained model file. It accepts input data through HTTP POST requests, \n",
        "makes predictions using the loaded model, and returns the results.\n",
        "\n",
        "Functions:\n",
        "- load_model_config: Loads the model configuration (training parameters) from a YAML configuration file.\n",
        "- run_app: Runs the FastAPI server in a separate thread to serve predictions.\n",
        "- predict: A POST endpoint that takes input features, processes them, and returns the model's prediction.\n",
        "\n",
        "Classes:\n",
        "- InputData: A Pydantic model for validating the structure of the incoming data.\n",
        "\n",
        "The FastAPI server listens on port 8000 and provides a `/predict/` endpoint for model inference. The server is run in \n",
        "a separate thread to allow asynchronous execution of requests.\n",
        "\n",
        "Requirements:\n",
        "- FastAPI for serving the model as an API.\n",
        "- PyTorch for loading the model and making predictions.\n",
        "- YAML for loading configuration from the 'config' file.\n",
        "\"\"\"\n",
        "\n",
        "# Load model configuration from the config file\n",
        "def load_model_config():\n",
        "    with open('config', 'r') as file:  # Load 'config' file\n",
        "        config = yaml.safe_load(file)\n",
        "    return config['training']\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Define a Pydantic model for input data\n",
        "class InputData(BaseModel):\n",
        "    features: list\n",
        "\n",
        "# Load model parameters from the config file\n",
        "model_config = load_model_config()\n",
        "\n",
        "input_size = model_config['input_size']\n",
        "output_size = model_config['output_size']\n",
        "hidden_sizes = model_config['hidden_sizes']\n",
        "initialization = model_config['initialization']\n",
        "\n",
        "# Create the model instance\n",
        "model = NNModel(input_size, hidden_sizes, output_size, initialization)\n",
        "model.load_state_dict(torch.load('model_weights.pth'))  # Load pre-trained weights\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "@app.post(\"/predict/\")\n",
        "async def predict(data: InputData):\n",
        "    # Convert input data to tensor and ensure it has the correct shape\n",
        "    input_tensor = torch.tensor(data.features, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
        "    with torch.no_grad():\n",
        "        prediction = model(input_tensor)\n",
        "    return {\"prediction\": prediction.tolist()}\n",
        "\n",
        "# Function to run the app in a separate thread\n",
        "def run_app():\n",
        "    run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "\n",
        "# Start the FastAPI server in a separate thread\n",
        "thread = threading.Thread(target=run_app)\n",
        "thread.start()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span style=\"font-size:18px; color:#007acc;\"><b> 7. Inference from API <a id=\"Inference from API\"></a> ##    "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This script extracts a single example from the test dataset, sends it to a FastAPI model server for prediction, and \n",
        "prints both the predicted and true labels. The steps include:\n",
        "\n",
        "1. Extract the first example (feature set) from the input tensor and the corresponding true label.\n",
        "2. Send a POST request to the FastAPI endpoint with the feature data.\n",
        "3. Process the response, which is assumed to contain logits from the model, apply the sigmoid activation function \n",
        "   to get probabilities, and convert them into binary predictions (0 or 1).\n",
        "4. Print the status code and response from the server, as well as the predicted labels and true label for comparison.\n",
        "\n",
        "Error handling is included for failed requests and invalid JSON responses.\n",
        "\n",
        "Dependencies:\n",
        "- requests for making HTTP requests.\n",
        "- The sigmoid function to convert logits into probabilities.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Extract the first example (row) from test_input_tensor and convert it to a list\n",
        "features = test_input_tensor[:1].tolist()[0]  # Extracts the first example and converts it to a list\n",
        "\n",
        "# Extract the true label corresponding to the first example\n",
        "true_label = test_output_tensor[0].item()  # Converts the tensor value to a Python scalar\n",
        "\n",
        "# Define the endpoint\n",
        "url = \"http://localhost:8000/predict/\"\n",
        "\n",
        "data = {\"features\": features}  # Use the extracted features\n",
        "\n",
        "try:\n",
        "    # Make the POST request\n",
        "    response = requests.post(url, json=data)\n",
        "\n",
        "    # Print status code\n",
        "    print(f\"Status Code: {response.status_code}\")\n",
        "\n",
        "    # Print response text for debugging\n",
        "    print(f\"Response Text: {response.text}\")\n",
        "\n",
        "    # Assuming the response is a JSON array of logits\n",
        "    logits = response.json()\n",
        "\n",
        "    # Apply the sigmoid function and threshold to each logit\n",
        "    prediction = sigmoid(logits)\n",
        "    prediction = [1 if logit >= 0.5 else 0 for logit in logits]\n",
        "\n",
        "    # Print the resulting predictions (0 or 1)\n",
        "    print(\"Predicted Labels:\", prediction)\n",
        "\n",
        "    # Print the true label\n",
        "    print(\"True Label:\", true_label)\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Request failed: {e}\")\n",
        "except ValueError as e:\n",
        "    print(f\"JSON decode error: {e}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}